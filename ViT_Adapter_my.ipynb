{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akianfar/ViT-Adapter/blob/main/ViT_Adapter_my.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare VIT-Adapter"
      ],
      "metadata": {
        "id": "XXfNpRe6pBnn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eUI0y13Vuv_i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "321325ff-dffd-4535-ca76-6f30b775e6ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ViT-Adapter'...\n",
            "remote: Enumerating objects: 1071, done.\u001b[K\n",
            "remote: Counting objects: 100% (1071/1071), done.\u001b[K\n",
            "remote: Compressing objects: 100% (430/430), done.\u001b[K\n",
            "remote: Total 1071 (delta 669), reused 1022 (delta 633), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1071/1071), 2.03 MiB | 11.80 MiB/s, done.\n",
            "Resolving deltas: 100% (669/669), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/akianfar/ViT-Adapter.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ViT-Adapter/segmentation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBFj2-46ZI0U",
        "outputId": "2716cb12-71c4-40f9-fd51-13d28a5b9fc9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ViT-Adapter/segmentation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# recommended environment: torch1.9 + cuda11.1\n",
        "!pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install mmcv-full==1.4.2 -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.9.0/index.html\n",
        "!pip install timm==0.4.12\n",
        "!pip install mmdet==2.22.0 # for Mask2Former\n",
        "!pip install mmsegmentation==0.20.2\n",
        "!ln -s ../detection/ops ./"
      ],
      "metadata": {
        "id": "k1XB2745pFpq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e05f9010-91f3-4063-9930-db7cfc60f89a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.9.0+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torch-1.9.0%2Bcu111-cp39-cp39-linux_x86_64.whl (2041.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m858.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.10.0+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.10.0%2Bcu111-cp39-cp39-linux_x86_64.whl (23.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.1/23.1 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==0.9.0\n",
            "  Downloading torchaudio-0.9.0-cp39-cp39-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.9.0+cu111) (4.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision==0.10.0+cu111) (1.22.4)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision==0.10.0+cu111) (8.4.0)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.0+cu118\n",
            "    Uninstalling torch-2.0.0+cu118:\n",
            "      Successfully uninstalled torch-2.0.0+cu118\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.15.1+cu118\n",
            "    Uninstalling torchvision-0.15.1+cu118:\n",
            "      Successfully uninstalled torchvision-0.15.1+cu118\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.0.1+cu118\n",
            "    Uninstalling torchaudio-2.0.1+cu118:\n",
            "      Successfully uninstalled torchaudio-2.0.1+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.15.1 requires torch==2.0.0, but you have torch 1.9.0+cu111 which is incompatible.\n",
            "torchdata 0.6.0 requires torch==2.0.0, but you have torch 1.9.0+cu111 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.9.0+cu111 torchaudio-0.9.0 torchvision-0.10.0+cu111\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.openmmlab.com/mmcv/dist/cu111/torch1.9.0/index.html\n",
            "Collecting mmcv-full==1.4.2\n",
            "  Downloading https://download.openmmlab.com/mmcv/dist/cu111/torch1.9.0/mmcv_full-1.4.2-cp39-cp39-manylinux1_x86_64.whl (60.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from mmcv-full==1.4.2) (23.0)\n",
            "Requirement already satisfied: opencv-python>=3 in /usr/local/lib/python3.9/dist-packages (from mmcv-full==1.4.2) (4.7.0.72)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.9/dist-packages (from mmcv-full==1.4.2) (8.4.0)\n",
            "Collecting yapf\n",
            "  Downloading yapf-0.32.0-py2.py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.2/190.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from mmcv-full==1.4.2) (6.0)\n",
            "Collecting addict\n",
            "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from mmcv-full==1.4.2) (1.22.4)\n",
            "Installing collected packages: yapf, addict, mmcv-full\n",
            "Successfully installed addict-2.4.0 mmcv-full-1.4.2 yapf-0.32.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting timm==0.4.12\n",
            "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from timm==0.4.12) (0.10.0+cu111)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.9/dist-packages (from timm==0.4.12) (1.9.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.4->timm==0.4.12) (4.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision->timm==0.4.12) (1.22.4)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->timm==0.4.12) (8.4.0)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.4.12\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mmdet==2.22.0\n",
            "  Downloading mmdet-2.22.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from mmdet==2.22.0) (1.16.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.9/dist-packages (from mmdet==2.22.0) (2.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from mmdet==2.22.0) (1.22.4)\n",
            "Collecting terminaltables\n",
            "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from mmdet==2.22.0) (3.7.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mmdet==2.22.0) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mmdet==2.22.0) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mmdet==2.22.0) (0.11.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mmdet==2.22.0) (1.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mmdet==2.22.0) (2.8.2)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mmdet==2.22.0) (5.12.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mmdet==2.22.0) (23.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mmdet==2.22.0) (8.4.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mmdet==2.22.0) (4.39.3)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->mmdet==2.22.0) (3.15.0)\n",
            "Installing collected packages: terminaltables, mmdet\n",
            "Successfully installed mmdet-2.22.0 terminaltables-3.1.10\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mmsegmentation==0.20.2\n",
            "  Downloading mmsegmentation-0.20.2-py3-none-any.whl (686 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m686.4/686.4 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from mmsegmentation==0.20.2) (23.0)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.9/dist-packages (from mmsegmentation==0.20.2) (0.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from mmsegmentation==0.20.2) (1.22.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from mmsegmentation==0.20.2) (3.7.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mmsegmentation==0.20.2) (0.11.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mmsegmentation==0.20.2) (1.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mmsegmentation==0.20.2) (2.8.2)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mmsegmentation==0.20.2) (5.12.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mmsegmentation==0.20.2) (4.39.3)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mmsegmentation==0.20.2) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mmsegmentation==0.20.2) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mmsegmentation==0.20.2) (1.4.4)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->mmsegmentation==0.20.2) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib->mmsegmentation==0.20.2) (1.16.0)\n",
            "Installing collected packages: mmsegmentation\n",
            "Successfully installed mmsegmentation-0.20.2\n",
            "ln: failed to create symbolic link './ops': File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ops \n",
        "!sh make.sh # compile deformable attention\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnRwTRXpwYOm",
        "outputId": "8d771d17-c9ea-4a1f-e978-bac49c8a90b7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ViT-Adapter/segmentation/ops\n",
            "running build\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib.linux-x86_64-3.9\n",
            "creating build/lib.linux-x86_64-3.9/functions\n",
            "copying functions/__init__.py -> build/lib.linux-x86_64-3.9/functions\n",
            "copying functions/ms_deform_attn_func.py -> build/lib.linux-x86_64-3.9/functions\n",
            "creating build/lib.linux-x86_64-3.9/modules\n",
            "copying modules/ms_deform_attn.py -> build/lib.linux-x86_64-3.9/modules\n",
            "copying modules/__init__.py -> build/lib.linux-x86_64-3.9/modules\n",
            "running build_ext\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/cpp_extension.py:370: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n",
            "building 'MultiScaleDeformableAttention' extension\n",
            "creating build/temp.linux-x86_64-3.9\n",
            "creating build/temp.linux-x86_64-3.9/content\n",
            "creating build/temp.linux-x86_64-3.9/content/ViT-Adapter\n",
            "creating build/temp.linux-x86_64-3.9/content/ViT-Adapter/segmentation\n",
            "creating build/temp.linux-x86_64-3.9/content/ViT-Adapter/segmentation/ops\n",
            "creating build/temp.linux-x86_64-3.9/content/ViT-Adapter/segmentation/ops/src\n",
            "creating build/temp.linux-x86_64-3.9/content/ViT-Adapter/segmentation/ops/src/cpu\n",
            "creating build/temp.linux-x86_64-3.9/content/ViT-Adapter/segmentation/ops/src/cuda\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DWITH_CUDA -I/content/ViT-Adapter/segmentation/ops/src -I/usr/local/lib/python3.9/dist-packages/torch/include -I/usr/local/lib/python3.9/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/dist-packages/torch/include/TH -I/usr/local/lib/python3.9/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.9 -c /content/ViT-Adapter/segmentation/ops/src/cpu/ms_deform_attn_cpu.cpp -o build/temp.linux-x86_64-3.9/content/ViT-Adapter/segmentation/ops/src/cpu/ms_deform_attn_cpu.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=MultiScaleDeformableAttention -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "/usr/local/cuda/bin/nvcc -DWITH_CUDA -I/content/ViT-Adapter/segmentation/ops/src -I/usr/local/lib/python3.9/dist-packages/torch/include -I/usr/local/lib/python3.9/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/dist-packages/torch/include/TH -I/usr/local/lib/python3.9/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.9 -c /content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu -o build/temp.linux-x86_64-3.9/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=MultiScaleDeformableAttention -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_im2col_cuda.cuh(261)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"q_col\"\u001b[0m was declared but never referenced\n",
            "          detected during instantiation of \u001b[01m\"void ms_deformable_im2col_cuda(cudaStream_t, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *) [with scalar_t=double]\"\u001b[0m \u001b[32m\n",
            "/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu(64): here\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_im2col_cuda.cuh(762)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"q_col\"\u001b[0m was declared but never referenced\n",
            "          detected during instantiation of \u001b[01m\"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\"\u001b[0m \u001b[32m\n",
            "/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu(134): here\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_im2col_cuda.cuh(872)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"q_col\"\u001b[0m was declared but never referenced\n",
            "          detected during instantiation of \u001b[01m\"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\"\u001b[0m \u001b[32m\n",
            "/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu(134): here\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_im2col_cuda.cuh(331)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"q_col\"\u001b[0m was declared but never referenced\n",
            "          detected during instantiation of \u001b[01m\"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\"\u001b[0m \u001b[32m\n",
            "/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu(134): here\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_im2col_cuda.cuh(436)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"q_col\"\u001b[0m was declared but never referenced\n",
            "          detected during instantiation of \u001b[01m\"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\"\u001b[0m \u001b[32m\n",
            "/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu(134): here\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_im2col_cuda.cuh(544)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"q_col\"\u001b[0m was declared but never referenced\n",
            "          detected during instantiation of \u001b[01m\"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\"\u001b[0m \u001b[32m\n",
            "/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu(134): here\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_im2col_cuda.cuh(649)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"q_col\"\u001b[0m was declared but never referenced\n",
            "          detected during instantiation of \u001b[01m\"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\"\u001b[0m \u001b[32m\n",
            "/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu(134): here\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kat::Tensor ms_deform_attn_cuda_forward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:34:62:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   34 |     AT_ASSERTM(value.type().is_cuda(), \"value must be a CUDA \u001b[01;35m\u001b[Kt\u001b[m\u001b[Kensor\");\n",
            "      |                                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:338:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  338 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:35:71:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   35 |     AT_ASSERTM(spatial_shapes.type().is_cuda(), \"spatial_shapes must b\u001b[01;35m\u001b[Ke\u001b[m\u001b[K a CUDA tensor\");\n",
            "      |                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:338:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  338 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:36:74:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   36 |     AT_ASSERTM(level_start_index.type().is_cuda(), \"level_start_index mus\u001b[01;35m\u001b[Kt\u001b[m\u001b[K be a CUDA tensor\");\n",
            "      |                                                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:338:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  338 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:37:69:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   37 |     AT_ASSERTM(sampling_loc.type().is_cuda(), \"sampling_loc must be \u001b[01;35m\u001b[Ka\u001b[m\u001b[K CUDA tensor\");\n",
            "      |                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:338:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  338 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:38:68:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   38 |     AT_ASSERTM(attn_weight.type().is_cuda(), \"attn_weight must be a\u001b[01;35m\u001b[K \u001b[m\u001b[KCUDA tensor\");\n",
            "      |                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:338:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  338 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:64:43:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.t\u001b[01;35m\u001b[Ky\u001b[m\u001b[Kpe(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:338:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  338 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:64:98:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/Dispatch.h:121:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  121 | \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "      | \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:64:98:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/Dispatch.h:121:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  121 | \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "      | \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:64:822:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:501:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  501 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:64:908:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:501:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  501 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:64:951:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:501:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  501 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:64:984:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:501:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  501 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:64:1067:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:501:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  501 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:64:1225:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:501:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  501 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:64:1938:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:501:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  501 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:64:2024:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:501:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  501 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:64:2067:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:501:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  501 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:64:2099:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:501:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  501 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:64:2181:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:501:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  501 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:64:2338:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:501:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  501 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kstd::vector<at::Tensor> ms_deform_attn_cuda_backward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:100:62:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  100 |     AT_ASSERTM(value.type().is_cuda(), \"value must be a CUDA \u001b[01;35m\u001b[Kt\u001b[m\u001b[Kensor\");\n",
            "      |                                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:338:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  338 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:101:71:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  101 |     AT_ASSERTM(spatial_shapes.type().is_cuda(), \"spatial_shapes must b\u001b[01;35m\u001b[Ke\u001b[m\u001b[K a CUDA tensor\");\n",
            "      |                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:338:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  338 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:102:74:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  102 |     AT_ASSERTM(level_start_index.type().is_cuda(), \"level_start_index mus\u001b[01;35m\u001b[Kt\u001b[m\u001b[K be a CUDA tensor\");\n",
            "      |                                                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:338:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  338 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:103:69:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  103 |     AT_ASSERTM(sampling_loc.type().is_cuda(), \"sampling_loc must be \u001b[01;35m\u001b[Ka\u001b[m\u001b[K CUDA tensor\");\n",
            "      |                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:338:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  338 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:104:68:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  104 |     AT_ASSERTM(attn_weight.type().is_cuda(), \"attn_weight must be a\u001b[01;35m\u001b[K \u001b[m\u001b[KCUDA tensor\");\n",
            "      |                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:338:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  338 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:105:68:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  105 |     AT_ASSERTM(grad_output.type().is_cuda(), \"grad_output must be a\u001b[01;35m\u001b[K \u001b[m\u001b[KCUDA tensor\");\n",
            "      |                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:338:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  338 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:134:43:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.t\u001b[01;35m\u001b[Ky\u001b[m\u001b[Kpe(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:338:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  338 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:134:98:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/Dispatch.h:121:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  121 | \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "      | \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:134:98:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/Dispatch.h:121:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  121 | \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "      | \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:134:832:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:501:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  501 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:134:858:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:501:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  501 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:134:944:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:501:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  501 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:134:987:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:501:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  501 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:134:1020:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:501:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  501 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:134:1103:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:501:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  501 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:134:1264:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:501:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  501 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:134:1348:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:501:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  501 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:134:1436:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:501:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  501 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:134:2211:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:501:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  501 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:134:2236:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:501:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  501 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:134:2322:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:501:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  501 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:134:2365:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:501:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  501 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:134:2397:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:501:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  501 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:134:2479:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:501:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  501 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:134:2639:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:501:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  501 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:134:2722:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:501:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  501 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.cu:134:2809:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:501:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  501 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DWITH_CUDA -I/content/ViT-Adapter/segmentation/ops/src -I/usr/local/lib/python3.9/dist-packages/torch/include -I/usr/local/lib/python3.9/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/dist-packages/torch/include/TH -I/usr/local/lib/python3.9/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.9 -c /content/ViT-Adapter/segmentation/ops/src/vision.cpp -o build/temp.linux-x86_64-3.9/content/ViT-Adapter/segmentation/ops/src/vision.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=MultiScaleDeformableAttention -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/Parallel.h:140\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cpu/ms_deform_attn_cpu.h:12\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/ms_deform_attn.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/vision.cpp:11\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/ParallelOpenMP.h:87:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring #pragma omp parallel [\u001b[01;35m\u001b[K-Wunknown-pragmas\u001b[m\u001b[K]\n",
            "   87 | #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "      | \n",
            "In file included from \u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/vision.cpp:11\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/ms_deform_attn.h:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kat::Tensor ms_deform_attn_forward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/ms_deform_attn.h:29:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   29 |     if (value.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda())\n",
            "      |                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cpu/ms_deform_attn_cpu.h:12\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/ms_deform_attn.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/vision.cpp:11\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:338:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  338 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/vision.cpp:11\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/ms_deform_attn.h:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kstd::vector<at::Tensor> ms_deform_attn_backward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/ms_deform_attn.h:51:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   51 |     if (value.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda())\n",
            "      |                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/cpu/ms_deform_attn_cpu.h:12\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/ms_deform_attn.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/ViT-Adapter/segmentation/ops/src/vision.cpp:11\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:338:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  338 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.9/content/ViT-Adapter/segmentation/ops/src/cpu/ms_deform_attn_cpu.o build/temp.linux-x86_64-3.9/content/ViT-Adapter/segmentation/ops/src/cuda/ms_deform_attn_cuda.o build/temp.linux-x86_64-3.9/content/ViT-Adapter/segmentation/ops/src/vision.o -L/usr/local/lib/python3.9/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.9/MultiScaleDeformableAttention.cpython-39-x86_64-linux-gnu.so\n",
            "running install\n",
            "/usr/local/lib/python3.9/dist-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/setuptools/command/easy_install.py:144: EasyInstallDeprecationWarning: easy_install command is deprecated. Use build and pip and other standards-based tools.\n",
            "  warnings.warn(\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating MultiScaleDeformableAttention.egg-info\n",
            "writing MultiScaleDeformableAttention.egg-info/PKG-INFO\n",
            "writing dependency_links to MultiScaleDeformableAttention.egg-info/dependency_links.txt\n",
            "writing top-level names to MultiScaleDeformableAttention.egg-info/top_level.txt\n",
            "writing manifest file 'MultiScaleDeformableAttention.egg-info/SOURCES.txt'\n",
            "reading manifest file 'MultiScaleDeformableAttention.egg-info/SOURCES.txt'\n",
            "writing manifest file 'MultiScaleDeformableAttention.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-3.9/MultiScaleDeformableAttention.cpython-39-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/functions\n",
            "copying build/lib.linux-x86_64-3.9/functions/__init__.py -> build/bdist.linux-x86_64/egg/functions\n",
            "copying build/lib.linux-x86_64-3.9/functions/ms_deform_attn_func.py -> build/bdist.linux-x86_64/egg/functions\n",
            "creating build/bdist.linux-x86_64/egg/modules\n",
            "copying build/lib.linux-x86_64-3.9/modules/ms_deform_attn.py -> build/bdist.linux-x86_64/egg/modules\n",
            "copying build/lib.linux-x86_64-3.9/modules/__init__.py -> build/bdist.linux-x86_64/egg/modules\n",
            "byte-compiling build/bdist.linux-x86_64/egg/functions/__init__.py to __init__.cpython-39.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/functions/ms_deform_attn_func.py to ms_deform_attn_func.cpython-39.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/modules/ms_deform_attn.py to ms_deform_attn.cpython-39.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/modules/__init__.py to __init__.cpython-39.pyc\n",
            "creating stub loader for MultiScaleDeformableAttention.cpython-39-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/MultiScaleDeformableAttention.py to MultiScaleDeformableAttention.cpython-39.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "__pycache__.MultiScaleDeformableAttention.cpython-39: module references __file__\n",
            "creating dist\n",
            "creating 'dist/MultiScaleDeformableAttention-1.0-py3.9-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing MultiScaleDeformableAttention-1.0-py3.9-linux-x86_64.egg\n",
            "creating /usr/local/lib/python3.9/dist-packages/MultiScaleDeformableAttention-1.0-py3.9-linux-x86_64.egg\n",
            "Extracting MultiScaleDeformableAttention-1.0-py3.9-linux-x86_64.egg to /usr/local/lib/python3.9/dist-packages\n",
            "Adding MultiScaleDeformableAttention 1.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.9/dist-packages/MultiScaleDeformableAttention-1.0-py3.9-linux-x86_64.egg\n",
            "Processing dependencies for MultiScaleDeformableAttention==1.0\n",
            "Finished processing dependencies for MultiScaleDeformableAttention==1.0\n",
            "/content/ViT-Adapter/segmentation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetch checkpoint\n",
        "\n"
      ],
      "metadata": {
        "id": "8DFixzWhpIBx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jPDkDnFsxw8",
        "outputId": "c1f67a20-6bd7-4c7a-f295-7f6be3a79acf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ViT-Adapter/segmentation\n",
            "/content/ViT-Adapter/segmentation/checkpoints\n",
            "--2023-04-18 13:11:38--  https://github.com/czczup/ViT-Adapter/releases/download/v0.3.1/upernet_augreg_adapter_tiny_512_160_ade20k.pth.tar\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/492936445/07a629f0-0067-43d0-bb87-adf5fabaf5b0?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230418%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230418T131138Z&X-Amz-Expires=300&X-Amz-Signature=9fd89559d30d24f4cd465b9e19c3bce75879e815779c944b5cf062578751396a&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=492936445&response-content-disposition=attachment%3B%20filename%3Dupernet_augreg_adapter_tiny_512_160_ade20k.pth.tar&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-04-18 13:11:38--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/492936445/07a629f0-0067-43d0-bb87-adf5fabaf5b0?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230418%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230418T131138Z&X-Amz-Expires=300&X-Amz-Signature=9fd89559d30d24f4cd465b9e19c3bce75879e815779c944b5cf062578751396a&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=492936445&response-content-disposition=attachment%3B%20filename%3Dupernet_augreg_adapter_tiny_512_160_ade20k.pth.tar&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 144467049 (138M) [application/octet-stream]\n",
            "Saving to: ‘upernet_augreg_adapter_tiny_512_160_ade20k.pth.tar’\n",
            "\n",
            "upernet_augreg_adap 100%[===================>] 137.77M  65.9MB/s    in 2.1s    \n",
            "\n",
            "2023-04-18 13:11:41 (65.9 MB/s) - ‘upernet_augreg_adapter_tiny_512_160_ade20k.pth.tar’ saved [144467049/144467049]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%cd /content/ViT-Adapter/segmentation\n",
        "!mkdir checkpoints\n",
        "%cd checkpoints\n",
        "!wget -c https://github.com/czczup/ViT-Adapter/releases/download/v0.3.1/upernet_augreg_adapter_tiny_512_160_ade20k.pth.tar"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Sclera dataset"
      ],
      "metadata": {
        "id": "sY3skBMZpUTR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4iKzgVJbPQuX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58c8ed0f-b477-4b3c-e4d6-015d4e53907f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ViT-Adapter/segmentation\n",
            "/content/ViT-Adapter/segmentation/data\n",
            "/content/ViT-Adapter/segmentation/data/ade\n"
          ]
        }
      ],
      "source": [
        "%cd /content/ViT-Adapter/segmentation\n",
        "!mkdir data\n",
        "%cd data\n",
        "!mkdir ade\n",
        "%cd ade\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference\n",
        "Run segmentation inference on a single image from Sclera class."
      ],
      "metadata": {
        "id": "MvMfo5VApj_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ViT-Adapter/segmentation"
      ],
      "metadata": {
        "id": "qeX03MbP8oPa",
        "outputId": "8ec1f43d-86e8-41ea-fbe3-47da0b680fd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ViT-Adapter/segmentation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ViT-Adapter/segmentation\n",
        "!python train.py configs/ade20k/upernet_deit_adapter_tiny_512_160k_ade20k.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VF2UUZjwI65h",
        "outputId": "aebac612-384a-4d47-b665-465637a8f231"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ViT-Adapter/segmentation\n",
            "2023-04-18 14:49:56,959 - mmseg - INFO - Environment info:\n",
            "------------------------------------------------------------\n",
            "sys.platform: linux\n",
            "Python: 3.9.16 (main, Dec  7 2022, 01:11:51) [GCC 9.4.0]\n",
            "CUDA available: True\n",
            "GPU 0: Tesla T4\n",
            "CUDA_HOME: /usr/local/cuda\n",
            "NVCC: Build cuda_11.8.r11.8/compiler.31833905_0\n",
            "GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
            "PyTorch: 1.9.0+cu111\n",
            "PyTorch compiling details: PyTorch built with:\n",
            "  - GCC 7.3\n",
            "  - C++ Version: 201402\n",
            "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX2\n",
            "  - CUDA Runtime 11.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86\n",
            "  - CuDNN 8.0.5\n",
            "  - Magma 2.5.2\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, \n",
            "\n",
            "TorchVision: 0.10.0+cu111\n",
            "OpenCV: 4.7.0\n",
            "MMCV: 1.4.2\n",
            "MMCV Compiler: GCC 7.3\n",
            "MMCV CUDA Compiler: 11.1\n",
            "MMSegmentation: 0.20.2+3f13a4c\n",
            "------------------------------------------------------------\n",
            "\n",
            "2023-04-18 14:49:56,959 - mmseg - INFO - Distributed training: False\n",
            "2023-04-18 14:49:57,483 - mmseg - INFO - Config:\n",
            "norm_cfg = dict(type='SyncBN', requires_grad=True)\n",
            "model = dict(\n",
            "    type='EncoderDecoder',\n",
            "    pretrained=\n",
            "    'https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth',\n",
            "    backbone=dict(\n",
            "        type='ViTAdapter',\n",
            "        patch_size=16,\n",
            "        embed_dim=192,\n",
            "        depth=12,\n",
            "        num_heads=3,\n",
            "        mlp_ratio=4,\n",
            "        drop_path_rate=0.1,\n",
            "        conv_inplane=64,\n",
            "        n_points=4,\n",
            "        deform_num_heads=6,\n",
            "        cffn_ratio=0.25,\n",
            "        deform_ratio=1.0,\n",
            "        interaction_indexes=[[0, 2], [3, 5], [6, 8], [9, 11]],\n",
            "        window_attn=[\n",
            "            False, False, False, False, False, False, False, False, False,\n",
            "            False, False, False\n",
            "        ],\n",
            "        window_size=[\n",
            "            None, None, None, None, None, None, None, None, None, None, None,\n",
            "            None\n",
            "        ]),\n",
            "    decode_head=dict(\n",
            "        type='UPerHead',\n",
            "        in_channels=[192, 192, 192, 192],\n",
            "        in_index=[0, 1, 2, 3],\n",
            "        pool_scales=(1, 2, 3, 6),\n",
            "        channels=512,\n",
            "        dropout_ratio=0.1,\n",
            "        num_classes=2,\n",
            "        norm_cfg=dict(type='SyncBN', requires_grad=True),\n",
            "        align_corners=False,\n",
            "        loss_decode=dict(\n",
            "            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0)),\n",
            "    auxiliary_head=dict(\n",
            "        type='FCNHead',\n",
            "        in_channels=192,\n",
            "        in_index=2,\n",
            "        channels=256,\n",
            "        num_convs=1,\n",
            "        concat_input=False,\n",
            "        dropout_ratio=0.1,\n",
            "        num_classes=2,\n",
            "        norm_cfg=dict(type='SyncBN', requires_grad=True),\n",
            "        align_corners=False,\n",
            "        loss_decode=dict(\n",
            "            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=0.4)),\n",
            "    train_cfg=dict(),\n",
            "    test_cfg=dict(mode='slide', crop_size=(512, 512), stride=(341, 341)))\n",
            "dataset_type = 'ADE20KDataset'\n",
            "data_root = 'data/ade'\n",
            "img_norm_cfg = dict(\n",
            "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
            "crop_size = (512, 512)\n",
            "train_pipeline = [\n",
            "    dict(type='LoadImageFromFile'),\n",
            "    dict(type='LoadAnnotations', reduce_zero_label=True),\n",
            "    dict(type='Resize', img_scale=(2048, 512), ratio_range=(0.5, 2.0)),\n",
            "    dict(type='RandomCrop', crop_size=(512, 512), cat_max_ratio=0.75),\n",
            "    dict(type='RandomFlip', prob=0.5),\n",
            "    dict(type='PhotoMetricDistortion'),\n",
            "    dict(\n",
            "        type='Normalize',\n",
            "        mean=[123.675, 116.28, 103.53],\n",
            "        std=[58.395, 57.12, 57.375],\n",
            "        to_rgb=True),\n",
            "    dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),\n",
            "    dict(type='DefaultFormatBundle'),\n",
            "    dict(type='Collect', keys=['img', 'gt_semantic_seg'])\n",
            "]\n",
            "test_pipeline = [\n",
            "    dict(type='LoadImageFromFile'),\n",
            "    dict(\n",
            "        type='MultiScaleFlipAug',\n",
            "        img_scale=(2048, 512),\n",
            "        flip=False,\n",
            "        transforms=[\n",
            "            dict(type='Resize', keep_ratio=True),\n",
            "            dict(type='ResizeToMultiple', size_divisor=32),\n",
            "            dict(type='RandomFlip'),\n",
            "            dict(\n",
            "                type='Normalize',\n",
            "                mean=[123.675, 116.28, 103.53],\n",
            "                std=[58.395, 57.12, 57.375],\n",
            "                to_rgb=True),\n",
            "            dict(type='ImageToTensor', keys=['img']),\n",
            "            dict(type='Collect', keys=['img'])\n",
            "        ])\n",
            "]\n",
            "data = dict(\n",
            "    samples_per_gpu=2,\n",
            "    workers_per_gpu=4,\n",
            "    train=dict(\n",
            "        type='ADE20KDataset',\n",
            "        data_root='data/ade',\n",
            "        img_dir='images/training',\n",
            "        ann_dir='annotations/training',\n",
            "        pipeline=[\n",
            "            dict(type='LoadImageFromFile'),\n",
            "            dict(type='LoadAnnotations', reduce_zero_label=True),\n",
            "            dict(type='Resize', img_scale=(2048, 512), ratio_range=(0.5, 2.0)),\n",
            "            dict(type='RandomCrop', crop_size=(512, 512), cat_max_ratio=0.75),\n",
            "            dict(type='RandomFlip', prob=0.5),\n",
            "            dict(type='PhotoMetricDistortion'),\n",
            "            dict(\n",
            "                type='Normalize',\n",
            "                mean=[123.675, 116.28, 103.53],\n",
            "                std=[58.395, 57.12, 57.375],\n",
            "                to_rgb=True),\n",
            "            dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),\n",
            "            dict(type='DefaultFormatBundle'),\n",
            "            dict(type='Collect', keys=['img', 'gt_semantic_seg'])\n",
            "        ]),\n",
            "    val=dict(\n",
            "        type='ADE20KDataset',\n",
            "        data_root='data/ade',\n",
            "        img_dir='images/validation',\n",
            "        ann_dir='annotations/validation',\n",
            "        pipeline=[\n",
            "            dict(type='LoadImageFromFile'),\n",
            "            dict(\n",
            "                type='MultiScaleFlipAug',\n",
            "                img_scale=(2048, 512),\n",
            "                flip=False,\n",
            "                transforms=[\n",
            "                    dict(type='Resize', keep_ratio=True),\n",
            "                    dict(type='ResizeToMultiple', size_divisor=32),\n",
            "                    dict(type='RandomFlip'),\n",
            "                    dict(\n",
            "                        type='Normalize',\n",
            "                        mean=[123.675, 116.28, 103.53],\n",
            "                        std=[58.395, 57.12, 57.375],\n",
            "                        to_rgb=True),\n",
            "                    dict(type='ImageToTensor', keys=['img']),\n",
            "                    dict(type='Collect', keys=['img'])\n",
            "                ])\n",
            "        ]),\n",
            "    test=dict(\n",
            "        type='ADE20KDataset',\n",
            "        data_root='data/ade',\n",
            "        img_dir='images/validation',\n",
            "        ann_dir='annotations/validation',\n",
            "        pipeline=[\n",
            "            dict(type='LoadImageFromFile'),\n",
            "            dict(\n",
            "                type='MultiScaleFlipAug',\n",
            "                img_scale=(2048, 512),\n",
            "                flip=False,\n",
            "                transforms=[\n",
            "                    dict(type='Resize', keep_ratio=True),\n",
            "                    dict(type='ResizeToMultiple', size_divisor=32),\n",
            "                    dict(type='RandomFlip'),\n",
            "                    dict(\n",
            "                        type='Normalize',\n",
            "                        mean=[123.675, 116.28, 103.53],\n",
            "                        std=[58.395, 57.12, 57.375],\n",
            "                        to_rgb=True),\n",
            "                    dict(type='ImageToTensor', keys=['img']),\n",
            "                    dict(type='Collect', keys=['img'])\n",
            "                ])\n",
            "        ]))\n",
            "log_config = dict(\n",
            "    interval=50, hooks=[dict(type='TextLoggerHook', by_epoch=False)])\n",
            "dist_params = dict(backend='nccl')\n",
            "log_level = 'INFO'\n",
            "load_from = None\n",
            "resume_from = None\n",
            "workflow = [('train', 1)]\n",
            "cudnn_benchmark = True\n",
            "optimizer = dict(\n",
            "    type='AdamW',\n",
            "    lr=0.00012,\n",
            "    betas=(0.9, 0.999),\n",
            "    weight_decay=0.01,\n",
            "    constructor='LayerDecayOptimizerConstructor',\n",
            "    paramwise_cfg=dict(num_layers=12, layer_decay_rate=0.95))\n",
            "optimizer_config = dict()\n",
            "lr_config = dict(\n",
            "    policy='poly',\n",
            "    warmup='linear',\n",
            "    warmup_iters=1500,\n",
            "    warmup_ratio=1e-06,\n",
            "    power=1.0,\n",
            "    min_lr=0.0,\n",
            "    by_epoch=False)\n",
            "runner = dict(type='IterBasedRunner', max_iters=400)\n",
            "checkpoint_config = dict(by_epoch=False, interval=1000, max_keep_ckpts=1)\n",
            "evaluation = dict(\n",
            "    interval=16000, metric='mIoU', pre_eval=True, save_best='mIoU')\n",
            "pretrained = 'https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth'\n",
            "fp16 = dict(loss_scale=dict(init_scale=512))\n",
            "work_dir = './work_dirs/upernet_deit_adapter_tiny_512_160k_ade20k'\n",
            "gpu_ids = range(0, 1)\n",
            "auto_resume = False\n",
            "\n",
            "2023-04-18 14:49:57,483 - mmseg - INFO - Set random seed to 1253623151, deterministic: False\n",
            "2023-04-18 14:49:57,566 - mmseg - WARNING - unexpected key in source state_dict: cls_token, norm.weight, norm.bias, head.weight, head.bias\n",
            "\n",
            "missing keys in source state_dict: blocks.0.gamma2, blocks.3.gamma1, blocks.4.gamma2, blocks.3.gamma2, blocks.9.gamma2, blocks.2.gamma2, blocks.8.gamma2, blocks.5.gamma2, blocks.9.gamma1, blocks.4.gamma1, blocks.11.gamma1, blocks.7.gamma1, blocks.7.gamma2, blocks.6.gamma1, blocks.6.gamma2, blocks.10.gamma2, blocks.11.gamma2, blocks.8.gamma1, blocks.1.gamma2, blocks.1.gamma1, blocks.10.gamma1, blocks.0.gamma1, blocks.2.gamma1, blocks.5.gamma1\n",
            "\n",
            "WARNING:mmseg:unexpected key in source state_dict: cls_token, norm.weight, norm.bias, head.weight, head.bias\n",
            "\n",
            "missing keys in source state_dict: blocks.0.gamma2, blocks.3.gamma1, blocks.4.gamma2, blocks.3.gamma2, blocks.9.gamma2, blocks.2.gamma2, blocks.8.gamma2, blocks.5.gamma2, blocks.9.gamma1, blocks.4.gamma1, blocks.11.gamma1, blocks.7.gamma1, blocks.7.gamma2, blocks.6.gamma1, blocks.6.gamma2, blocks.10.gamma2, blocks.11.gamma2, blocks.8.gamma1, blocks.1.gamma2, blocks.1.gamma1, blocks.10.gamma1, blocks.0.gamma1, blocks.2.gamma1, blocks.5.gamma1\n",
            "\n",
            "/content/ViT-Adapter/segmentation/mmseg_custom/models/losses/cross_entropy_loss.py:230: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with PyTorch official cross_entropy, set ``avg_non_ignore=True``.\n",
            "  warnings.warn(\n",
            "2023-04-18 14:49:58,044 - mmseg - INFO - initialize UPerHead with init_cfg {'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}\n",
            "INFO:mmseg:initialize UPerHead with init_cfg {'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}\n",
            "2023-04-18 14:49:58,206 - mmseg - INFO - initialize FCNHead with init_cfg {'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}\n",
            "INFO:mmseg:initialize FCNHead with init_cfg {'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}\n",
            "/content/ViT-Adapter/segmentation/train.py:179: UserWarning: SyncBN is only supported with DDP. To be compatible with DP, we convert SyncBN to BN. Please use dist_train.sh which can avoid this error.\n",
            "  warnings.warn(\n",
            "2023-04-18 14:49:58,214 - mmseg - INFO - EncoderDecoder(\n",
            "  (backbone): ViTAdapter(\n",
            "    (patch_embed): PatchEmbed(\n",
            "      (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
            "      (norm): Identity()\n",
            "    )\n",
            "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
            "    (blocks): Sequential(\n",
            "      (0): Block(\n",
            "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (1): Block(\n",
            "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (2): Block(\n",
            "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (3): Block(\n",
            "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (4): Block(\n",
            "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (5): Block(\n",
            "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (6): Block(\n",
            "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (7): Block(\n",
            "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (8): Block(\n",
            "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (9): Block(\n",
            "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (10): Block(\n",
            "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (11): Block(\n",
            "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (spm): SpatialPriorModule(\n",
            "      (stem): Sequential(\n",
            "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (1): _BatchNormXd(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (4): _BatchNormXd(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU(inplace=True)\n",
            "        (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (7): _BatchNormXd(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (8): ReLU(inplace=True)\n",
            "        (9): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "      )\n",
            "      (conv2): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (1): _BatchNormXd(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (conv3): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (1): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (conv4): Sequential(\n",
            "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (1): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (fc1): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (fc2): Conv2d(128, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (fc3): Conv2d(256, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (fc4): Conv2d(256, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (interactions): Sequential(\n",
            "      (0): InteractionBlock(\n",
            "        (injector): Injector(\n",
            "          (query_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (feat_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): MSDeformAttn(\n",
            "            (sampling_offsets): Linear(in_features=192, out_features=144, bias=True)\n",
            "            (attention_weights): Linear(in_features=192, out_features=72, bias=True)\n",
            "            (value_proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "            (output_proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (extractor): Extractor(\n",
            "          (query_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (feat_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): MSDeformAttn(\n",
            "            (sampling_offsets): Linear(in_features=192, out_features=48, bias=True)\n",
            "            (attention_weights): Linear(in_features=192, out_features=24, bias=True)\n",
            "            (value_proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "            (output_proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          )\n",
            "          (ffn): ConvFFN(\n",
            "            (fc1): Linear(in_features=192, out_features=48, bias=True)\n",
            "            (dwconv): DWConv(\n",
            "              (dwconv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)\n",
            "            )\n",
            "            (act): GELU()\n",
            "            (fc2): Linear(in_features=48, out_features=192, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ffn_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (drop_path): DropPath()\n",
            "        )\n",
            "      )\n",
            "      (1): InteractionBlock(\n",
            "        (injector): Injector(\n",
            "          (query_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (feat_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): MSDeformAttn(\n",
            "            (sampling_offsets): Linear(in_features=192, out_features=144, bias=True)\n",
            "            (attention_weights): Linear(in_features=192, out_features=72, bias=True)\n",
            "            (value_proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "            (output_proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (extractor): Extractor(\n",
            "          (query_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (feat_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): MSDeformAttn(\n",
            "            (sampling_offsets): Linear(in_features=192, out_features=48, bias=True)\n",
            "            (attention_weights): Linear(in_features=192, out_features=24, bias=True)\n",
            "            (value_proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "            (output_proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          )\n",
            "          (ffn): ConvFFN(\n",
            "            (fc1): Linear(in_features=192, out_features=48, bias=True)\n",
            "            (dwconv): DWConv(\n",
            "              (dwconv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)\n",
            "            )\n",
            "            (act): GELU()\n",
            "            (fc2): Linear(in_features=48, out_features=192, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ffn_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (drop_path): DropPath()\n",
            "        )\n",
            "      )\n",
            "      (2): InteractionBlock(\n",
            "        (injector): Injector(\n",
            "          (query_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (feat_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): MSDeformAttn(\n",
            "            (sampling_offsets): Linear(in_features=192, out_features=144, bias=True)\n",
            "            (attention_weights): Linear(in_features=192, out_features=72, bias=True)\n",
            "            (value_proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "            (output_proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (extractor): Extractor(\n",
            "          (query_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (feat_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): MSDeformAttn(\n",
            "            (sampling_offsets): Linear(in_features=192, out_features=48, bias=True)\n",
            "            (attention_weights): Linear(in_features=192, out_features=24, bias=True)\n",
            "            (value_proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "            (output_proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          )\n",
            "          (ffn): ConvFFN(\n",
            "            (fc1): Linear(in_features=192, out_features=48, bias=True)\n",
            "            (dwconv): DWConv(\n",
            "              (dwconv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)\n",
            "            )\n",
            "            (act): GELU()\n",
            "            (fc2): Linear(in_features=48, out_features=192, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ffn_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (drop_path): DropPath()\n",
            "        )\n",
            "      )\n",
            "      (3): InteractionBlock(\n",
            "        (injector): Injector(\n",
            "          (query_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (feat_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): MSDeformAttn(\n",
            "            (sampling_offsets): Linear(in_features=192, out_features=144, bias=True)\n",
            "            (attention_weights): Linear(in_features=192, out_features=72, bias=True)\n",
            "            (value_proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "            (output_proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (extractor): Extractor(\n",
            "          (query_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (feat_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): MSDeformAttn(\n",
            "            (sampling_offsets): Linear(in_features=192, out_features=48, bias=True)\n",
            "            (attention_weights): Linear(in_features=192, out_features=24, bias=True)\n",
            "            (value_proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "            (output_proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          )\n",
            "          (ffn): ConvFFN(\n",
            "            (fc1): Linear(in_features=192, out_features=48, bias=True)\n",
            "            (dwconv): DWConv(\n",
            "              (dwconv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)\n",
            "            )\n",
            "            (act): GELU()\n",
            "            (fc2): Linear(in_features=48, out_features=192, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ffn_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (drop_path): DropPath()\n",
            "        )\n",
            "        (extra_extractors): Sequential(\n",
            "          (0): Extractor(\n",
            "            (query_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "            (feat_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "            (attn): MSDeformAttn(\n",
            "              (sampling_offsets): Linear(in_features=192, out_features=48, bias=True)\n",
            "              (attention_weights): Linear(in_features=192, out_features=24, bias=True)\n",
            "              (value_proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "              (output_proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "            )\n",
            "            (ffn): ConvFFN(\n",
            "              (fc1): Linear(in_features=192, out_features=48, bias=True)\n",
            "              (dwconv): DWConv(\n",
            "                (dwconv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)\n",
            "              )\n",
            "              (act): GELU()\n",
            "              (fc2): Linear(in_features=48, out_features=192, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (ffn_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "            (drop_path): DropPath()\n",
            "          )\n",
            "          (1): Extractor(\n",
            "            (query_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "            (feat_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "            (attn): MSDeformAttn(\n",
            "              (sampling_offsets): Linear(in_features=192, out_features=48, bias=True)\n",
            "              (attention_weights): Linear(in_features=192, out_features=24, bias=True)\n",
            "              (value_proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "              (output_proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "            )\n",
            "            (ffn): ConvFFN(\n",
            "              (fc1): Linear(in_features=192, out_features=48, bias=True)\n",
            "              (dwconv): DWConv(\n",
            "                (dwconv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)\n",
            "              )\n",
            "              (act): GELU()\n",
            "              (fc2): Linear(in_features=48, out_features=192, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (ffn_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "            (drop_path): DropPath()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (up): ConvTranspose2d(192, 192, kernel_size=(2, 2), stride=(2, 2))\n",
            "    (norm1): _BatchNormXd(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (norm2): _BatchNormXd(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (norm3): _BatchNormXd(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (norm4): _BatchNormXd(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (decode_head): UPerHead(\n",
            "    input_transform=multiple_select, ignore_index=255, align_corners=False\n",
            "    (loss_decode): CrossEntropyLoss(avg_non_ignore=False)\n",
            "    (conv_seg): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
            "    (psp_modules): PPM(\n",
            "      (0): Sequential(\n",
            "        (0): AdaptiveAvgPool2d(output_size=1)\n",
            "        (1): ConvModule(\n",
            "          (conv): Conv2d(192, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (activate): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Sequential(\n",
            "        (0): AdaptiveAvgPool2d(output_size=2)\n",
            "        (1): ConvModule(\n",
            "          (conv): Conv2d(192, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (activate): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (2): Sequential(\n",
            "        (0): AdaptiveAvgPool2d(output_size=3)\n",
            "        (1): ConvModule(\n",
            "          (conv): Conv2d(192, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (activate): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (3): Sequential(\n",
            "        (0): AdaptiveAvgPool2d(output_size=6)\n",
            "        (1): ConvModule(\n",
            "          (conv): Conv2d(192, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (activate): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (bottleneck): ConvModule(\n",
            "      (conv): Conv2d(2240, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activate): ReLU(inplace=True)\n",
            "    )\n",
            "    (lateral_convs): ModuleList(\n",
            "      (0): ConvModule(\n",
            "        (conv): Conv2d(192, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (activate): ReLU()\n",
            "      )\n",
            "      (1): ConvModule(\n",
            "        (conv): Conv2d(192, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (activate): ReLU()\n",
            "      )\n",
            "      (2): ConvModule(\n",
            "        (conv): Conv2d(192, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (activate): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (fpn_convs): ModuleList(\n",
            "      (0): ConvModule(\n",
            "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (activate): ReLU()\n",
            "      )\n",
            "      (1): ConvModule(\n",
            "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (activate): ReLU()\n",
            "      )\n",
            "      (2): ConvModule(\n",
            "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (activate): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (fpn_bottleneck): ConvModule(\n",
            "      (conv): Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activate): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  init_cfg={'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}\n",
            "  (auxiliary_head): FCNHead(\n",
            "    input_transform=None, ignore_index=255, align_corners=False\n",
            "    (loss_decode): CrossEntropyLoss(avg_non_ignore=False)\n",
            "    (conv_seg): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
            "    (convs): Sequential(\n",
            "      (0): ConvModule(\n",
            "        (conv): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (activate): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  init_cfg={'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}\n",
            ")\n",
            "INFO:mmseg:EncoderDecoder(\n",
            "  (backbone): ViTAdapter(\n",
            "    (patch_embed): PatchEmbed(\n",
            "      (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
            "      (norm): Identity()\n",
            "    )\n",
            "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
            "    (blocks): Sequential(\n",
            "      (0): Block(\n",
            "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (1): Block(\n",
            "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (2): Block(\n",
            "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (3): Block(\n",
            "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (4): Block(\n",
            "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (5): Block(\n",
            "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (6): Block(\n",
            "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (7): Block(\n",
            "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (8): Block(\n",
            "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (9): Block(\n",
            "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (10): Block(\n",
            "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (11): Block(\n",
            "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "          (act): GELU()\n",
            "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (spm): SpatialPriorModule(\n",
            "      (stem): Sequential(\n",
            "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (1): _BatchNormXd(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (4): _BatchNormXd(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU(inplace=True)\n",
            "        (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (7): _BatchNormXd(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (8): ReLU(inplace=True)\n",
            "        (9): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "      )\n",
            "      (conv2): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (1): _BatchNormXd(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (conv3): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (1): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (conv4): Sequential(\n",
            "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (1): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (fc1): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (fc2): Conv2d(128, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (fc3): Conv2d(256, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (fc4): Conv2d(256, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (interactions): Sequential(\n",
            "      (0): InteractionBlock(\n",
            "        (injector): Injector(\n",
            "          (query_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (feat_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): MSDeformAttn(\n",
            "            (sampling_offsets): Linear(in_features=192, out_features=144, bias=True)\n",
            "            (attention_weights): Linear(in_features=192, out_features=72, bias=True)\n",
            "            (value_proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "            (output_proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (extractor): Extractor(\n",
            "          (query_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (feat_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): MSDeformAttn(\n",
            "            (sampling_offsets): Linear(in_features=192, out_features=48, bias=True)\n",
            "            (attention_weights): Linear(in_features=192, out_features=24, bias=True)\n",
            "            (value_proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "            (output_proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          )\n",
            "          (ffn): ConvFFN(\n",
            "            (fc1): Linear(in_features=192, out_features=48, bias=True)\n",
            "            (dwconv): DWConv(\n",
            "              (dwconv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)\n",
            "            )\n",
            "            (act): GELU()\n",
            "            (fc2): Linear(in_features=48, out_features=192, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ffn_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (drop_path): DropPath()\n",
            "        )\n",
            "      )\n",
            "      (1): InteractionBlock(\n",
            "        (injector): Injector(\n",
            "          (query_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (feat_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): MSDeformAttn(\n",
            "            (sampling_offsets): Linear(in_features=192, out_features=144, bias=True)\n",
            "            (attention_weights): Linear(in_features=192, out_features=72, bias=True)\n",
            "            (value_proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "            (output_proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (extractor): Extractor(\n",
            "          (query_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (feat_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): MSDeformAttn(\n",
            "            (sampling_offsets): Linear(in_features=192, out_features=48, bias=True)\n",
            "            (attention_weights): Linear(in_features=192, out_features=24, bias=True)\n",
            "            (value_proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "            (output_proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          )\n",
            "          (ffn): ConvFFN(\n",
            "            (fc1): Linear(in_features=192, out_features=48, bias=True)\n",
            "            (dwconv): DWConv(\n",
            "              (dwconv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)\n",
            "            )\n",
            "            (act): GELU()\n",
            "            (fc2): Linear(in_features=48, out_features=192, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ffn_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (drop_path): DropPath()\n",
            "        )\n",
            "      )\n",
            "      (2): InteractionBlock(\n",
            "        (injector): Injector(\n",
            "          (query_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (feat_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): MSDeformAttn(\n",
            "            (sampling_offsets): Linear(in_features=192, out_features=144, bias=True)\n",
            "            (attention_weights): Linear(in_features=192, out_features=72, bias=True)\n",
            "            (value_proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "            (output_proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (extractor): Extractor(\n",
            "          (query_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (feat_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): MSDeformAttn(\n",
            "            (sampling_offsets): Linear(in_features=192, out_features=48, bias=True)\n",
            "            (attention_weights): Linear(in_features=192, out_features=24, bias=True)\n",
            "            (value_proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "            (output_proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          )\n",
            "          (ffn): ConvFFN(\n",
            "            (fc1): Linear(in_features=192, out_features=48, bias=True)\n",
            "            (dwconv): DWConv(\n",
            "              (dwconv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)\n",
            "            )\n",
            "            (act): GELU()\n",
            "            (fc2): Linear(in_features=48, out_features=192, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ffn_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (drop_path): DropPath()\n",
            "        )\n",
            "      )\n",
            "      (3): InteractionBlock(\n",
            "        (injector): Injector(\n",
            "          (query_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (feat_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): MSDeformAttn(\n",
            "            (sampling_offsets): Linear(in_features=192, out_features=144, bias=True)\n",
            "            (attention_weights): Linear(in_features=192, out_features=72, bias=True)\n",
            "            (value_proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "            (output_proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (extractor): Extractor(\n",
            "          (query_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (feat_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): MSDeformAttn(\n",
            "            (sampling_offsets): Linear(in_features=192, out_features=48, bias=True)\n",
            "            (attention_weights): Linear(in_features=192, out_features=24, bias=True)\n",
            "            (value_proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "            (output_proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          )\n",
            "          (ffn): ConvFFN(\n",
            "            (fc1): Linear(in_features=192, out_features=48, bias=True)\n",
            "            (dwconv): DWConv(\n",
            "              (dwconv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)\n",
            "            )\n",
            "            (act): GELU()\n",
            "            (fc2): Linear(in_features=48, out_features=192, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ffn_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (drop_path): DropPath()\n",
            "        )\n",
            "        (extra_extractors): Sequential(\n",
            "          (0): Extractor(\n",
            "            (query_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "            (feat_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "            (attn): MSDeformAttn(\n",
            "              (sampling_offsets): Linear(in_features=192, out_features=48, bias=True)\n",
            "              (attention_weights): Linear(in_features=192, out_features=24, bias=True)\n",
            "              (value_proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "              (output_proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "            )\n",
            "            (ffn): ConvFFN(\n",
            "              (fc1): Linear(in_features=192, out_features=48, bias=True)\n",
            "              (dwconv): DWConv(\n",
            "                (dwconv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)\n",
            "              )\n",
            "              (act): GELU()\n",
            "              (fc2): Linear(in_features=48, out_features=192, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (ffn_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "            (drop_path): DropPath()\n",
            "          )\n",
            "          (1): Extractor(\n",
            "            (query_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "            (feat_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "            (attn): MSDeformAttn(\n",
            "              (sampling_offsets): Linear(in_features=192, out_features=48, bias=True)\n",
            "              (attention_weights): Linear(in_features=192, out_features=24, bias=True)\n",
            "              (value_proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "              (output_proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "            )\n",
            "            (ffn): ConvFFN(\n",
            "              (fc1): Linear(in_features=192, out_features=48, bias=True)\n",
            "              (dwconv): DWConv(\n",
            "                (dwconv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)\n",
            "              )\n",
            "              (act): GELU()\n",
            "              (fc2): Linear(in_features=48, out_features=192, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (ffn_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "            (drop_path): DropPath()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (up): ConvTranspose2d(192, 192, kernel_size=(2, 2), stride=(2, 2))\n",
            "    (norm1): _BatchNormXd(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (norm2): _BatchNormXd(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (norm3): _BatchNormXd(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (norm4): _BatchNormXd(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (decode_head): UPerHead(\n",
            "    input_transform=multiple_select, ignore_index=255, align_corners=False\n",
            "    (loss_decode): CrossEntropyLoss(avg_non_ignore=False)\n",
            "    (conv_seg): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
            "    (psp_modules): PPM(\n",
            "      (0): Sequential(\n",
            "        (0): AdaptiveAvgPool2d(output_size=1)\n",
            "        (1): ConvModule(\n",
            "          (conv): Conv2d(192, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (activate): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Sequential(\n",
            "        (0): AdaptiveAvgPool2d(output_size=2)\n",
            "        (1): ConvModule(\n",
            "          (conv): Conv2d(192, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (activate): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (2): Sequential(\n",
            "        (0): AdaptiveAvgPool2d(output_size=3)\n",
            "        (1): ConvModule(\n",
            "          (conv): Conv2d(192, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (activate): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (3): Sequential(\n",
            "        (0): AdaptiveAvgPool2d(output_size=6)\n",
            "        (1): ConvModule(\n",
            "          (conv): Conv2d(192, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (activate): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (bottleneck): ConvModule(\n",
            "      (conv): Conv2d(2240, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activate): ReLU(inplace=True)\n",
            "    )\n",
            "    (lateral_convs): ModuleList(\n",
            "      (0): ConvModule(\n",
            "        (conv): Conv2d(192, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (activate): ReLU()\n",
            "      )\n",
            "      (1): ConvModule(\n",
            "        (conv): Conv2d(192, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (activate): ReLU()\n",
            "      )\n",
            "      (2): ConvModule(\n",
            "        (conv): Conv2d(192, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (activate): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (fpn_convs): ModuleList(\n",
            "      (0): ConvModule(\n",
            "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (activate): ReLU()\n",
            "      )\n",
            "      (1): ConvModule(\n",
            "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (activate): ReLU()\n",
            "      )\n",
            "      (2): ConvModule(\n",
            "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (activate): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (fpn_bottleneck): ConvModule(\n",
            "      (conv): Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): _BatchNormXd(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activate): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  init_cfg={'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}\n",
            "  (auxiliary_head): FCNHead(\n",
            "    input_transform=None, ignore_index=255, align_corners=False\n",
            "    (loss_decode): CrossEntropyLoss(avg_non_ignore=False)\n",
            "    (conv_seg): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
            "    (convs): Sequential(\n",
            "      (0): ConvModule(\n",
            "        (conv): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (activate): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  init_cfg={'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}\n",
            ")\n",
            "2023-04-18 14:49:58,404 - mmseg - INFO - Loaded 5 images\n",
            "INFO:mmseg:Loaded 5 images\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "{'num_layers': 12, 'layer_decay_rate': 0.95}\n",
            "Build LayerDecayOptimizerConstructor 0.950000 - 14\n",
            "Param groups = {\n",
            "  \"layer_0_decay\": {\n",
            "    \"param_names\": [\n",
            "      \"backbone.pos_embed\",\n",
            "      \"backbone.patch_embed.proj.weight\"\n",
            "    ],\n",
            "    \"lr_scale\": 0.5133420832795048,\n",
            "    \"lr\": 6.160104999354058e-05,\n",
            "    \"weight_decay\": 0.01\n",
            "  },\n",
            "  \"layer_13_decay\": {\n",
            "    \"param_names\": [\n",
            "      \"backbone.level_embed\",\n",
            "      \"backbone.spm.stem.0.weight\",\n",
            "      \"backbone.spm.stem.3.weight\",\n",
            "      \"backbone.spm.stem.6.weight\",\n",
            "      \"backbone.spm.conv2.0.weight\",\n",
            "      \"backbone.spm.conv3.0.weight\",\n",
            "      \"backbone.spm.conv4.0.weight\",\n",
            "      \"backbone.spm.fc1.weight\",\n",
            "      \"backbone.spm.fc2.weight\",\n",
            "      \"backbone.spm.fc3.weight\",\n",
            "      \"backbone.spm.fc4.weight\",\n",
            "      \"backbone.interactions.0.injector.attn.sampling_offsets.weight\",\n",
            "      \"backbone.interactions.0.injector.attn.attention_weights.weight\",\n",
            "      \"backbone.interactions.0.injector.attn.value_proj.weight\",\n",
            "      \"backbone.interactions.0.injector.attn.output_proj.weight\",\n",
            "      \"backbone.interactions.0.extractor.attn.sampling_offsets.weight\",\n",
            "      \"backbone.interactions.0.extractor.attn.attention_weights.weight\",\n",
            "      \"backbone.interactions.0.extractor.attn.value_proj.weight\",\n",
            "      \"backbone.interactions.0.extractor.attn.output_proj.weight\",\n",
            "      \"backbone.interactions.0.extractor.ffn.fc1.weight\",\n",
            "      \"backbone.interactions.0.extractor.ffn.dwconv.dwconv.weight\",\n",
            "      \"backbone.interactions.0.extractor.ffn.fc2.weight\",\n",
            "      \"backbone.interactions.1.injector.attn.sampling_offsets.weight\",\n",
            "      \"backbone.interactions.1.injector.attn.attention_weights.weight\",\n",
            "      \"backbone.interactions.1.injector.attn.value_proj.weight\",\n",
            "      \"backbone.interactions.1.injector.attn.output_proj.weight\",\n",
            "      \"backbone.interactions.1.extractor.attn.sampling_offsets.weight\",\n",
            "      \"backbone.interactions.1.extractor.attn.attention_weights.weight\",\n",
            "      \"backbone.interactions.1.extractor.attn.value_proj.weight\",\n",
            "      \"backbone.interactions.1.extractor.attn.output_proj.weight\",\n",
            "      \"backbone.interactions.1.extractor.ffn.fc1.weight\",\n",
            "      \"backbone.interactions.1.extractor.ffn.dwconv.dwconv.weight\",\n",
            "      \"backbone.interactions.1.extractor.ffn.fc2.weight\",\n",
            "      \"backbone.interactions.2.injector.attn.sampling_offsets.weight\",\n",
            "      \"backbone.interactions.2.injector.attn.attention_weights.weight\",\n",
            "      \"backbone.interactions.2.injector.attn.value_proj.weight\",\n",
            "      \"backbone.interactions.2.injector.attn.output_proj.weight\",\n",
            "      \"backbone.interactions.2.extractor.attn.sampling_offsets.weight\",\n",
            "      \"backbone.interactions.2.extractor.attn.attention_weights.weight\",\n",
            "      \"backbone.interactions.2.extractor.attn.value_proj.weight\",\n",
            "      \"backbone.interactions.2.extractor.attn.output_proj.weight\",\n",
            "      \"backbone.interactions.2.extractor.ffn.fc1.weight\",\n",
            "      \"backbone.interactions.2.extractor.ffn.dwconv.dwconv.weight\",\n",
            "      \"backbone.interactions.2.extractor.ffn.fc2.weight\",\n",
            "      \"backbone.interactions.3.injector.attn.sampling_offsets.weight\",\n",
            "      \"backbone.interactions.3.injector.attn.attention_weights.weight\",\n",
            "      \"backbone.interactions.3.injector.attn.value_proj.weight\",\n",
            "      \"backbone.interactions.3.injector.attn.output_proj.weight\",\n",
            "      \"backbone.interactions.3.extractor.attn.sampling_offsets.weight\",\n",
            "      \"backbone.interactions.3.extractor.attn.attention_weights.weight\",\n",
            "      \"backbone.interactions.3.extractor.attn.value_proj.weight\",\n",
            "      \"backbone.interactions.3.extractor.attn.output_proj.weight\",\n",
            "      \"backbone.interactions.3.extractor.ffn.fc1.weight\",\n",
            "      \"backbone.interactions.3.extractor.ffn.dwconv.dwconv.weight\",\n",
            "      \"backbone.interactions.3.extractor.ffn.fc2.weight\",\n",
            "      \"backbone.interactions.3.extra_extractors.0.attn.sampling_offsets.weight\",\n",
            "      \"backbone.interactions.3.extra_extractors.0.attn.attention_weights.weight\",\n",
            "      \"backbone.interactions.3.extra_extractors.0.attn.value_proj.weight\",\n",
            "      \"backbone.interactions.3.extra_extractors.0.attn.output_proj.weight\",\n",
            "      \"backbone.interactions.3.extra_extractors.0.ffn.fc1.weight\",\n",
            "      \"backbone.interactions.3.extra_extractors.0.ffn.dwconv.dwconv.weight\",\n",
            "      \"backbone.interactions.3.extra_extractors.0.ffn.fc2.weight\",\n",
            "      \"backbone.interactions.3.extra_extractors.1.attn.sampling_offsets.weight\",\n",
            "      \"backbone.interactions.3.extra_extractors.1.attn.attention_weights.weight\",\n",
            "      \"backbone.interactions.3.extra_extractors.1.attn.value_proj.weight\",\n",
            "      \"backbone.interactions.3.extra_extractors.1.attn.output_proj.weight\",\n",
            "      \"backbone.interactions.3.extra_extractors.1.ffn.fc1.weight\",\n",
            "      \"backbone.interactions.3.extra_extractors.1.ffn.dwconv.dwconv.weight\",\n",
            "      \"backbone.interactions.3.extra_extractors.1.ffn.fc2.weight\",\n",
            "      \"backbone.up.weight\",\n",
            "      \"decode_head.conv_seg.weight\",\n",
            "      \"decode_head.psp_modules.0.1.conv.weight\",\n",
            "      \"decode_head.psp_modules.1.1.conv.weight\",\n",
            "      \"decode_head.psp_modules.2.1.conv.weight\",\n",
            "      \"decode_head.psp_modules.3.1.conv.weight\",\n",
            "      \"decode_head.bottleneck.conv.weight\",\n",
            "      \"decode_head.lateral_convs.0.conv.weight\",\n",
            "      \"decode_head.lateral_convs.1.conv.weight\",\n",
            "      \"decode_head.lateral_convs.2.conv.weight\",\n",
            "      \"decode_head.fpn_convs.0.conv.weight\",\n",
            "      \"decode_head.fpn_convs.1.conv.weight\",\n",
            "      \"decode_head.fpn_convs.2.conv.weight\",\n",
            "      \"decode_head.fpn_bottleneck.conv.weight\",\n",
            "      \"auxiliary_head.conv_seg.weight\",\n",
            "      \"auxiliary_head.convs.0.conv.weight\"\n",
            "    ],\n",
            "    \"lr_scale\": 1.0,\n",
            "    \"lr\": 0.00012,\n",
            "    \"weight_decay\": 0.01\n",
            "  },\n",
            "  \"layer_0_no_decay\": {\n",
            "    \"param_names\": [\n",
            "      \"backbone.patch_embed.proj.bias\"\n",
            "    ],\n",
            "    \"lr_scale\": 0.5133420832795048,\n",
            "    \"lr\": 6.160104999354058e-05,\n",
            "    \"weight_decay\": 0.0\n",
            "  },\n",
            "  \"layer_1_no_decay\": {\n",
            "    \"param_names\": [\n",
            "      \"backbone.blocks.0.gamma1\",\n",
            "      \"backbone.blocks.0.gamma2\",\n",
            "      \"backbone.blocks.0.norm1.weight\",\n",
            "      \"backbone.blocks.0.norm1.bias\",\n",
            "      \"backbone.blocks.0.attn.qkv.bias\",\n",
            "      \"backbone.blocks.0.attn.proj.bias\",\n",
            "      \"backbone.blocks.0.norm2.weight\",\n",
            "      \"backbone.blocks.0.norm2.bias\",\n",
            "      \"backbone.blocks.0.mlp.fc1.bias\",\n",
            "      \"backbone.blocks.0.mlp.fc2.bias\"\n",
            "    ],\n",
            "    \"lr_scale\": 0.5403600876626367,\n",
            "    \"lr\": 6.48432105195164e-05,\n",
            "    \"weight_decay\": 0.0\n",
            "  },\n",
            "  \"layer_1_decay\": {\n",
            "    \"param_names\": [\n",
            "      \"backbone.blocks.0.attn.qkv.weight\",\n",
            "      \"backbone.blocks.0.attn.proj.weight\",\n",
            "      \"backbone.blocks.0.mlp.fc1.weight\",\n",
            "      \"backbone.blocks.0.mlp.fc2.weight\"\n",
            "    ],\n",
            "    \"lr_scale\": 0.5403600876626367,\n",
            "    \"lr\": 6.48432105195164e-05,\n",
            "    \"weight_decay\": 0.01\n",
            "  },\n",
            "  \"layer_2_no_decay\": {\n",
            "    \"param_names\": [\n",
            "      \"backbone.blocks.1.gamma1\",\n",
            "      \"backbone.blocks.1.gamma2\",\n",
            "      \"backbone.blocks.1.norm1.weight\",\n",
            "      \"backbone.blocks.1.norm1.bias\",\n",
            "      \"backbone.blocks.1.attn.qkv.bias\",\n",
            "      \"backbone.blocks.1.attn.proj.bias\",\n",
            "      \"backbone.blocks.1.norm2.weight\",\n",
            "      \"backbone.blocks.1.norm2.bias\",\n",
            "      \"backbone.blocks.1.mlp.fc1.bias\",\n",
            "      \"backbone.blocks.1.mlp.fc2.bias\"\n",
            "    ],\n",
            "    \"lr_scale\": 0.5688000922764597,\n",
            "    \"lr\": 6.825601107317516e-05,\n",
            "    \"weight_decay\": 0.0\n",
            "  },\n",
            "  \"layer_2_decay\": {\n",
            "    \"param_names\": [\n",
            "      \"backbone.blocks.1.attn.qkv.weight\",\n",
            "      \"backbone.blocks.1.attn.proj.weight\",\n",
            "      \"backbone.blocks.1.mlp.fc1.weight\",\n",
            "      \"backbone.blocks.1.mlp.fc2.weight\"\n",
            "    ],\n",
            "    \"lr_scale\": 0.5688000922764597,\n",
            "    \"lr\": 6.825601107317516e-05,\n",
            "    \"weight_decay\": 0.01\n",
            "  },\n",
            "  \"layer_3_no_decay\": {\n",
            "    \"param_names\": [\n",
            "      \"backbone.blocks.2.gamma1\",\n",
            "      \"backbone.blocks.2.gamma2\",\n",
            "      \"backbone.blocks.2.norm1.weight\",\n",
            "      \"backbone.blocks.2.norm1.bias\",\n",
            "      \"backbone.blocks.2.attn.qkv.bias\",\n",
            "      \"backbone.blocks.2.attn.proj.bias\",\n",
            "      \"backbone.blocks.2.norm2.weight\",\n",
            "      \"backbone.blocks.2.norm2.bias\",\n",
            "      \"backbone.blocks.2.mlp.fc1.bias\",\n",
            "      \"backbone.blocks.2.mlp.fc2.bias\"\n",
            "    ],\n",
            "    \"lr_scale\": 0.5987369392383787,\n",
            "    \"lr\": 7.184843270860544e-05,\n",
            "    \"weight_decay\": 0.0\n",
            "  },\n",
            "  \"layer_3_decay\": {\n",
            "    \"param_names\": [\n",
            "      \"backbone.blocks.2.attn.qkv.weight\",\n",
            "      \"backbone.blocks.2.attn.proj.weight\",\n",
            "      \"backbone.blocks.2.mlp.fc1.weight\",\n",
            "      \"backbone.blocks.2.mlp.fc2.weight\"\n",
            "    ],\n",
            "    \"lr_scale\": 0.5987369392383787,\n",
            "    \"lr\": 7.184843270860544e-05,\n",
            "    \"weight_decay\": 0.01\n",
            "  },\n",
            "  \"layer_4_no_decay\": {\n",
            "    \"param_names\": [\n",
            "      \"backbone.blocks.3.gamma1\",\n",
            "      \"backbone.blocks.3.gamma2\",\n",
            "      \"backbone.blocks.3.norm1.weight\",\n",
            "      \"backbone.blocks.3.norm1.bias\",\n",
            "      \"backbone.blocks.3.attn.qkv.bias\",\n",
            "      \"backbone.blocks.3.attn.proj.bias\",\n",
            "      \"backbone.blocks.3.norm2.weight\",\n",
            "      \"backbone.blocks.3.norm2.bias\",\n",
            "      \"backbone.blocks.3.mlp.fc1.bias\",\n",
            "      \"backbone.blocks.3.mlp.fc2.bias\"\n",
            "    ],\n",
            "    \"lr_scale\": 0.6302494097246091,\n",
            "    \"lr\": 7.562992916695309e-05,\n",
            "    \"weight_decay\": 0.0\n",
            "  },\n",
            "  \"layer_4_decay\": {\n",
            "    \"param_names\": [\n",
            "      \"backbone.blocks.3.attn.qkv.weight\",\n",
            "      \"backbone.blocks.3.attn.proj.weight\",\n",
            "      \"backbone.blocks.3.mlp.fc1.weight\",\n",
            "      \"backbone.blocks.3.mlp.fc2.weight\"\n",
            "    ],\n",
            "    \"lr_scale\": 0.6302494097246091,\n",
            "    \"lr\": 7.562992916695309e-05,\n",
            "    \"weight_decay\": 0.01\n",
            "  },\n",
            "  \"layer_5_no_decay\": {\n",
            "    \"param_names\": [\n",
            "      \"backbone.blocks.4.gamma1\",\n",
            "      \"backbone.blocks.4.gamma2\",\n",
            "      \"backbone.blocks.4.norm1.weight\",\n",
            "      \"backbone.blocks.4.norm1.bias\",\n",
            "      \"backbone.blocks.4.attn.qkv.bias\",\n",
            "      \"backbone.blocks.4.attn.proj.bias\",\n",
            "      \"backbone.blocks.4.norm2.weight\",\n",
            "      \"backbone.blocks.4.norm2.bias\",\n",
            "      \"backbone.blocks.4.mlp.fc1.bias\",\n",
            "      \"backbone.blocks.4.mlp.fc2.bias\"\n",
            "    ],\n",
            "    \"lr_scale\": 0.6634204312890623,\n",
            "    \"lr\": 7.961045175468747e-05,\n",
            "    \"weight_decay\": 0.0\n",
            "  },\n",
            "  \"layer_5_decay\": {\n",
            "    \"param_names\": [\n",
            "      \"backbone.blocks.4.attn.qkv.weight\",\n",
            "      \"backbone.blocks.4.attn.proj.weight\",\n",
            "      \"backbone.blocks.4.mlp.fc1.weight\",\n",
            "      \"backbone.blocks.4.mlp.fc2.weight\"\n",
            "    ],\n",
            "    \"lr_scale\": 0.6634204312890623,\n",
            "    \"lr\": 7.961045175468747e-05,\n",
            "    \"weight_decay\": 0.01\n",
            "  },\n",
            "  \"layer_6_no_decay\": {\n",
            "    \"param_names\": [\n",
            "      \"backbone.blocks.5.gamma1\",\n",
            "      \"backbone.blocks.5.gamma2\",\n",
            "      \"backbone.blocks.5.norm1.weight\",\n",
            "      \"backbone.blocks.5.norm1.bias\",\n",
            "      \"backbone.blocks.5.attn.qkv.bias\",\n",
            "      \"backbone.blocks.5.attn.proj.bias\",\n",
            "      \"backbone.blocks.5.norm2.weight\",\n",
            "      \"backbone.blocks.5.norm2.bias\",\n",
            "      \"backbone.blocks.5.mlp.fc1.bias\",\n",
            "      \"backbone.blocks.5.mlp.fc2.bias\"\n",
            "    ],\n",
            "    \"lr_scale\": 0.6983372960937497,\n",
            "    \"lr\": 8.380047553124997e-05,\n",
            "    \"weight_decay\": 0.0\n",
            "  },\n",
            "  \"layer_6_decay\": {\n",
            "    \"param_names\": [\n",
            "      \"backbone.blocks.5.attn.qkv.weight\",\n",
            "      \"backbone.blocks.5.attn.proj.weight\",\n",
            "      \"backbone.blocks.5.mlp.fc1.weight\",\n",
            "      \"backbone.blocks.5.mlp.fc2.weight\"\n",
            "    ],\n",
            "    \"lr_scale\": 0.6983372960937497,\n",
            "    \"lr\": 8.380047553124997e-05,\n",
            "    \"weight_decay\": 0.01\n",
            "  },\n",
            "  \"layer_7_no_decay\": {\n",
            "    \"param_names\": [\n",
            "      \"backbone.blocks.6.gamma1\",\n",
            "      \"backbone.blocks.6.gamma2\",\n",
            "      \"backbone.blocks.6.norm1.weight\",\n",
            "      \"backbone.blocks.6.norm1.bias\",\n",
            "      \"backbone.blocks.6.attn.qkv.bias\",\n",
            "      \"backbone.blocks.6.attn.proj.bias\",\n",
            "      \"backbone.blocks.6.norm2.weight\",\n",
            "      \"backbone.blocks.6.norm2.bias\",\n",
            "      \"backbone.blocks.6.mlp.fc1.bias\",\n",
            "      \"backbone.blocks.6.mlp.fc2.bias\"\n",
            "    ],\n",
            "    \"lr_scale\": 0.7350918906249998,\n",
            "    \"lr\": 8.821102687499997e-05,\n",
            "    \"weight_decay\": 0.0\n",
            "  },\n",
            "  \"layer_7_decay\": {\n",
            "    \"param_names\": [\n",
            "      \"backbone.blocks.6.attn.qkv.weight\",\n",
            "      \"backbone.blocks.6.attn.proj.weight\",\n",
            "      \"backbone.blocks.6.mlp.fc1.weight\",\n",
            "      \"backbone.blocks.6.mlp.fc2.weight\"\n",
            "    ],\n",
            "    \"lr_scale\": 0.7350918906249998,\n",
            "    \"lr\": 8.821102687499997e-05,\n",
            "    \"weight_decay\": 0.01\n",
            "  },\n",
            "  \"layer_8_no_decay\": {\n",
            "    \"param_names\": [\n",
            "      \"backbone.blocks.7.gamma1\",\n",
            "      \"backbone.blocks.7.gamma2\",\n",
            "      \"backbone.blocks.7.norm1.weight\",\n",
            "      \"backbone.blocks.7.norm1.bias\",\n",
            "      \"backbone.blocks.7.attn.qkv.bias\",\n",
            "      \"backbone.blocks.7.attn.proj.bias\",\n",
            "      \"backbone.blocks.7.norm2.weight\",\n",
            "      \"backbone.blocks.7.norm2.bias\",\n",
            "      \"backbone.blocks.7.mlp.fc1.bias\",\n",
            "      \"backbone.blocks.7.mlp.fc2.bias\"\n",
            "    ],\n",
            "    \"lr_scale\": 0.7737809374999998,\n",
            "    \"lr\": 9.285371249999997e-05,\n",
            "    \"weight_decay\": 0.0\n",
            "  },\n",
            "  \"layer_8_decay\": {\n",
            "    \"param_names\": [\n",
            "      \"backbone.blocks.7.attn.qkv.weight\",\n",
            "      \"backbone.blocks.7.attn.proj.weight\",\n",
            "      \"backbone.blocks.7.mlp.fc1.weight\",\n",
            "      \"backbone.blocks.7.mlp.fc2.weight\"\n",
            "    ],\n",
            "    \"lr_scale\": 0.7737809374999998,\n",
            "    \"lr\": 9.285371249999997e-05,\n",
            "    \"weight_decay\": 0.01\n",
            "  },\n",
            "  \"layer_9_no_decay\": {\n",
            "    \"param_names\": [\n",
            "      \"backbone.blocks.8.gamma1\",\n",
            "      \"backbone.blocks.8.gamma2\",\n",
            "      \"backbone.blocks.8.norm1.weight\",\n",
            "      \"backbone.blocks.8.norm1.bias\",\n",
            "      \"backbone.blocks.8.attn.qkv.bias\",\n",
            "      \"backbone.blocks.8.attn.proj.bias\",\n",
            "      \"backbone.blocks.8.norm2.weight\",\n",
            "      \"backbone.blocks.8.norm2.bias\",\n",
            "      \"backbone.blocks.8.mlp.fc1.bias\",\n",
            "      \"backbone.blocks.8.mlp.fc2.bias\"\n",
            "    ],\n",
            "    \"lr_scale\": 0.8145062499999999,\n",
            "    \"lr\": 9.774075e-05,\n",
            "    \"weight_decay\": 0.0\n",
            "  },\n",
            "  \"layer_9_decay\": {\n",
            "    \"param_names\": [\n",
            "      \"backbone.blocks.8.attn.qkv.weight\",\n",
            "      \"backbone.blocks.8.attn.proj.weight\",\n",
            "      \"backbone.blocks.8.mlp.fc1.weight\",\n",
            "      \"backbone.blocks.8.mlp.fc2.weight\"\n",
            "    ],\n",
            "    \"lr_scale\": 0.8145062499999999,\n",
            "    \"lr\": 9.774075e-05,\n",
            "    \"weight_decay\": 0.01\n",
            "  },\n",
            "  \"layer_10_no_decay\": {\n",
            "    \"param_names\": [\n",
            "      \"backbone.blocks.9.gamma1\",\n",
            "      \"backbone.blocks.9.gamma2\",\n",
            "      \"backbone.blocks.9.norm1.weight\",\n",
            "      \"backbone.blocks.9.norm1.bias\",\n",
            "      \"backbone.blocks.9.attn.qkv.bias\",\n",
            "      \"backbone.blocks.9.attn.proj.bias\",\n",
            "      \"backbone.blocks.9.norm2.weight\",\n",
            "      \"backbone.blocks.9.norm2.bias\",\n",
            "      \"backbone.blocks.9.mlp.fc1.bias\",\n",
            "      \"backbone.blocks.9.mlp.fc2.bias\"\n",
            "    ],\n",
            "    \"lr_scale\": 0.8573749999999999,\n",
            "    \"lr\": 0.00010288499999999999,\n",
            "    \"weight_decay\": 0.0\n",
            "  },\n",
            "  \"layer_10_decay\": {\n",
            "    \"param_names\": [\n",
            "      \"backbone.blocks.9.attn.qkv.weight\",\n",
            "      \"backbone.blocks.9.attn.proj.weight\",\n",
            "      \"backbone.blocks.9.mlp.fc1.weight\",\n",
            "      \"backbone.blocks.9.mlp.fc2.weight\"\n",
            "    ],\n",
            "    \"lr_scale\": 0.8573749999999999,\n",
            "    \"lr\": 0.00010288499999999999,\n",
            "    \"weight_decay\": 0.01\n",
            "  },\n",
            "  \"layer_11_no_decay\": {\n",
            "    \"param_names\": [\n",
            "      \"backbone.blocks.10.gamma1\",\n",
            "      \"backbone.blocks.10.gamma2\",\n",
            "      \"backbone.blocks.10.norm1.weight\",\n",
            "      \"backbone.blocks.10.norm1.bias\",\n",
            "      \"backbone.blocks.10.attn.qkv.bias\",\n",
            "      \"backbone.blocks.10.attn.proj.bias\",\n",
            "      \"backbone.blocks.10.norm2.weight\",\n",
            "      \"backbone.blocks.10.norm2.bias\",\n",
            "      \"backbone.blocks.10.mlp.fc1.bias\",\n",
            "      \"backbone.blocks.10.mlp.fc2.bias\"\n",
            "    ],\n",
            "    \"lr_scale\": 0.9025,\n",
            "    \"lr\": 0.0001083,\n",
            "    \"weight_decay\": 0.0\n",
            "  },\n",
            "  \"layer_11_decay\": {\n",
            "    \"param_names\": [\n",
            "      \"backbone.blocks.10.attn.qkv.weight\",\n",
            "      \"backbone.blocks.10.attn.proj.weight\",\n",
            "      \"backbone.blocks.10.mlp.fc1.weight\",\n",
            "      \"backbone.blocks.10.mlp.fc2.weight\"\n",
            "    ],\n",
            "    \"lr_scale\": 0.9025,\n",
            "    \"lr\": 0.0001083,\n",
            "    \"weight_decay\": 0.01\n",
            "  },\n",
            "  \"layer_12_no_decay\": {\n",
            "    \"param_names\": [\n",
            "      \"backbone.blocks.11.gamma1\",\n",
            "      \"backbone.blocks.11.gamma2\",\n",
            "      \"backbone.blocks.11.norm1.weight\",\n",
            "      \"backbone.blocks.11.norm1.bias\",\n",
            "      \"backbone.blocks.11.attn.qkv.bias\",\n",
            "      \"backbone.blocks.11.attn.proj.bias\",\n",
            "      \"backbone.blocks.11.norm2.weight\",\n",
            "      \"backbone.blocks.11.norm2.bias\",\n",
            "      \"backbone.blocks.11.mlp.fc1.bias\",\n",
            "      \"backbone.blocks.11.mlp.fc2.bias\"\n",
            "    ],\n",
            "    \"lr_scale\": 0.95,\n",
            "    \"lr\": 0.00011399999999999999,\n",
            "    \"weight_decay\": 0.0\n",
            "  },\n",
            "  \"layer_12_decay\": {\n",
            "    \"param_names\": [\n",
            "      \"backbone.blocks.11.attn.qkv.weight\",\n",
            "      \"backbone.blocks.11.attn.proj.weight\",\n",
            "      \"backbone.blocks.11.mlp.fc1.weight\",\n",
            "      \"backbone.blocks.11.mlp.fc2.weight\"\n",
            "    ],\n",
            "    \"lr_scale\": 0.95,\n",
            "    \"lr\": 0.00011399999999999999,\n",
            "    \"weight_decay\": 0.01\n",
            "  },\n",
            "  \"layer_13_no_decay\": {\n",
            "    \"param_names\": [\n",
            "      \"backbone.spm.stem.1.weight\",\n",
            "      \"backbone.spm.stem.1.bias\",\n",
            "      \"backbone.spm.stem.4.weight\",\n",
            "      \"backbone.spm.stem.4.bias\",\n",
            "      \"backbone.spm.stem.7.weight\",\n",
            "      \"backbone.spm.stem.7.bias\",\n",
            "      \"backbone.spm.conv2.1.weight\",\n",
            "      \"backbone.spm.conv2.1.bias\",\n",
            "      \"backbone.spm.conv3.1.weight\",\n",
            "      \"backbone.spm.conv3.1.bias\",\n",
            "      \"backbone.spm.conv4.1.weight\",\n",
            "      \"backbone.spm.conv4.1.bias\",\n",
            "      \"backbone.spm.fc1.bias\",\n",
            "      \"backbone.spm.fc2.bias\",\n",
            "      \"backbone.spm.fc3.bias\",\n",
            "      \"backbone.spm.fc4.bias\",\n",
            "      \"backbone.interactions.0.injector.gamma\",\n",
            "      \"backbone.interactions.0.injector.query_norm.weight\",\n",
            "      \"backbone.interactions.0.injector.query_norm.bias\",\n",
            "      \"backbone.interactions.0.injector.feat_norm.weight\",\n",
            "      \"backbone.interactions.0.injector.feat_norm.bias\",\n",
            "      \"backbone.interactions.0.injector.attn.sampling_offsets.bias\",\n",
            "      \"backbone.interactions.0.injector.attn.attention_weights.bias\",\n",
            "      \"backbone.interactions.0.injector.attn.value_proj.bias\",\n",
            "      \"backbone.interactions.0.injector.attn.output_proj.bias\",\n",
            "      \"backbone.interactions.0.extractor.query_norm.weight\",\n",
            "      \"backbone.interactions.0.extractor.query_norm.bias\",\n",
            "      \"backbone.interactions.0.extractor.feat_norm.weight\",\n",
            "      \"backbone.interactions.0.extractor.feat_norm.bias\",\n",
            "      \"backbone.interactions.0.extractor.attn.sampling_offsets.bias\",\n",
            "      \"backbone.interactions.0.extractor.attn.attention_weights.bias\",\n",
            "      \"backbone.interactions.0.extractor.attn.value_proj.bias\",\n",
            "      \"backbone.interactions.0.extractor.attn.output_proj.bias\",\n",
            "      \"backbone.interactions.0.extractor.ffn.fc1.bias\",\n",
            "      \"backbone.interactions.0.extractor.ffn.dwconv.dwconv.bias\",\n",
            "      \"backbone.interactions.0.extractor.ffn.fc2.bias\",\n",
            "      \"backbone.interactions.0.extractor.ffn_norm.weight\",\n",
            "      \"backbone.interactions.0.extractor.ffn_norm.bias\",\n",
            "      \"backbone.interactions.1.injector.gamma\",\n",
            "      \"backbone.interactions.1.injector.query_norm.weight\",\n",
            "      \"backbone.interactions.1.injector.query_norm.bias\",\n",
            "      \"backbone.interactions.1.injector.feat_norm.weight\",\n",
            "      \"backbone.interactions.1.injector.feat_norm.bias\",\n",
            "      \"backbone.interactions.1.injector.attn.sampling_offsets.bias\",\n",
            "      \"backbone.interactions.1.injector.attn.attention_weights.bias\",\n",
            "      \"backbone.interactions.1.injector.attn.value_proj.bias\",\n",
            "      \"backbone.interactions.1.injector.attn.output_proj.bias\",\n",
            "      \"backbone.interactions.1.extractor.query_norm.weight\",\n",
            "      \"backbone.interactions.1.extractor.query_norm.bias\",\n",
            "      \"backbone.interactions.1.extractor.feat_norm.weight\",\n",
            "      \"backbone.interactions.1.extractor.feat_norm.bias\",\n",
            "      \"backbone.interactions.1.extractor.attn.sampling_offsets.bias\",\n",
            "      \"backbone.interactions.1.extractor.attn.attention_weights.bias\",\n",
            "      \"backbone.interactions.1.extractor.attn.value_proj.bias\",\n",
            "      \"backbone.interactions.1.extractor.attn.output_proj.bias\",\n",
            "      \"backbone.interactions.1.extractor.ffn.fc1.bias\",\n",
            "      \"backbone.interactions.1.extractor.ffn.dwconv.dwconv.bias\",\n",
            "      \"backbone.interactions.1.extractor.ffn.fc2.bias\",\n",
            "      \"backbone.interactions.1.extractor.ffn_norm.weight\",\n",
            "      \"backbone.interactions.1.extractor.ffn_norm.bias\",\n",
            "      \"backbone.interactions.2.injector.gamma\",\n",
            "      \"backbone.interactions.2.injector.query_norm.weight\",\n",
            "      \"backbone.interactions.2.injector.query_norm.bias\",\n",
            "      \"backbone.interactions.2.injector.feat_norm.weight\",\n",
            "      \"backbone.interactions.2.injector.feat_norm.bias\",\n",
            "      \"backbone.interactions.2.injector.attn.sampling_offsets.bias\",\n",
            "      \"backbone.interactions.2.injector.attn.attention_weights.bias\",\n",
            "      \"backbone.interactions.2.injector.attn.value_proj.bias\",\n",
            "      \"backbone.interactions.2.injector.attn.output_proj.bias\",\n",
            "      \"backbone.interactions.2.extractor.query_norm.weight\",\n",
            "      \"backbone.interactions.2.extractor.query_norm.bias\",\n",
            "      \"backbone.interactions.2.extractor.feat_norm.weight\",\n",
            "      \"backbone.interactions.2.extractor.feat_norm.bias\",\n",
            "      \"backbone.interactions.2.extractor.attn.sampling_offsets.bias\",\n",
            "      \"backbone.interactions.2.extractor.attn.attention_weights.bias\",\n",
            "      \"backbone.interactions.2.extractor.attn.value_proj.bias\",\n",
            "      \"backbone.interactions.2.extractor.attn.output_proj.bias\",\n",
            "      \"backbone.interactions.2.extractor.ffn.fc1.bias\",\n",
            "      \"backbone.interactions.2.extractor.ffn.dwconv.dwconv.bias\",\n",
            "      \"backbone.interactions.2.extractor.ffn.fc2.bias\",\n",
            "      \"backbone.interactions.2.extractor.ffn_norm.weight\",\n",
            "      \"backbone.interactions.2.extractor.ffn_norm.bias\",\n",
            "      \"backbone.interactions.3.injector.gamma\",\n",
            "      \"backbone.interactions.3.injector.query_norm.weight\",\n",
            "      \"backbone.interactions.3.injector.query_norm.bias\",\n",
            "      \"backbone.interactions.3.injector.feat_norm.weight\",\n",
            "      \"backbone.interactions.3.injector.feat_norm.bias\",\n",
            "      \"backbone.interactions.3.injector.attn.sampling_offsets.bias\",\n",
            "      \"backbone.interactions.3.injector.attn.attention_weights.bias\",\n",
            "      \"backbone.interactions.3.injector.attn.value_proj.bias\",\n",
            "      \"backbone.interactions.3.injector.attn.output_proj.bias\",\n",
            "      \"backbone.interactions.3.extractor.query_norm.weight\",\n",
            "      \"backbone.interactions.3.extractor.query_norm.bias\",\n",
            "      \"backbone.interactions.3.extractor.feat_norm.weight\",\n",
            "      \"backbone.interactions.3.extractor.feat_norm.bias\",\n",
            "      \"backbone.interactions.3.extractor.attn.sampling_offsets.bias\",\n",
            "      \"backbone.interactions.3.extractor.attn.attention_weights.bias\",\n",
            "      \"backbone.interactions.3.extractor.attn.value_proj.bias\",\n",
            "      \"backbone.interactions.3.extractor.attn.output_proj.bias\",\n",
            "      \"backbone.interactions.3.extractor.ffn.fc1.bias\",\n",
            "      \"backbone.interactions.3.extractor.ffn.dwconv.dwconv.bias\",\n",
            "      \"backbone.interactions.3.extractor.ffn.fc2.bias\",\n",
            "      \"backbone.interactions.3.extractor.ffn_norm.weight\",\n",
            "      \"backbone.interactions.3.extractor.ffn_norm.bias\",\n",
            "      \"backbone.interactions.3.extra_extractors.0.query_norm.weight\",\n",
            "      \"backbone.interactions.3.extra_extractors.0.query_norm.bias\",\n",
            "      \"backbone.interactions.3.extra_extractors.0.feat_norm.weight\",\n",
            "      \"backbone.interactions.3.extra_extractors.0.feat_norm.bias\",\n",
            "      \"backbone.interactions.3.extra_extractors.0.attn.sampling_offsets.bias\",\n",
            "      \"backbone.interactions.3.extra_extractors.0.attn.attention_weights.bias\",\n",
            "      \"backbone.interactions.3.extra_extractors.0.attn.value_proj.bias\",\n",
            "      \"backbone.interactions.3.extra_extractors.0.attn.output_proj.bias\",\n",
            "      \"backbone.interactions.3.extra_extractors.0.ffn.fc1.bias\",\n",
            "      \"backbone.interactions.3.extra_extractors.0.ffn.dwconv.dwconv.bias\",\n",
            "      \"backbone.interactions.3.extra_extractors.0.ffn.fc2.bias\",\n",
            "      \"backbone.interactions.3.extra_extractors.0.ffn_norm.weight\",\n",
            "      \"backbone.interactions.3.extra_extractors.0.ffn_norm.bias\",\n",
            "      \"backbone.interactions.3.extra_extractors.1.query_norm.weight\",\n",
            "      \"backbone.interactions.3.extra_extractors.1.query_norm.bias\",\n",
            "      \"backbone.interactions.3.extra_extractors.1.feat_norm.weight\",\n",
            "      \"backbone.interactions.3.extra_extractors.1.feat_norm.bias\",\n",
            "      \"backbone.interactions.3.extra_extractors.1.attn.sampling_offsets.bias\",\n",
            "      \"backbone.interactions.3.extra_extractors.1.attn.attention_weights.bias\",\n",
            "      \"backbone.interactions.3.extra_extractors.1.attn.value_proj.bias\",\n",
            "      \"backbone.interactions.3.extra_extractors.1.attn.output_proj.bias\",\n",
            "      \"backbone.interactions.3.extra_extractors.1.ffn.fc1.bias\",\n",
            "      \"backbone.interactions.3.extra_extractors.1.ffn.dwconv.dwconv.bias\",\n",
            "      \"backbone.interactions.3.extra_extractors.1.ffn.fc2.bias\",\n",
            "      \"backbone.interactions.3.extra_extractors.1.ffn_norm.weight\",\n",
            "      \"backbone.interactions.3.extra_extractors.1.ffn_norm.bias\",\n",
            "      \"backbone.up.bias\",\n",
            "      \"backbone.norm1.weight\",\n",
            "      \"backbone.norm1.bias\",\n",
            "      \"backbone.norm2.weight\",\n",
            "      \"backbone.norm2.bias\",\n",
            "      \"backbone.norm3.weight\",\n",
            "      \"backbone.norm3.bias\",\n",
            "      \"backbone.norm4.weight\",\n",
            "      \"backbone.norm4.bias\",\n",
            "      \"decode_head.conv_seg.bias\",\n",
            "      \"decode_head.psp_modules.0.1.bn.weight\",\n",
            "      \"decode_head.psp_modules.0.1.bn.bias\",\n",
            "      \"decode_head.psp_modules.1.1.bn.weight\",\n",
            "      \"decode_head.psp_modules.1.1.bn.bias\",\n",
            "      \"decode_head.psp_modules.2.1.bn.weight\",\n",
            "      \"decode_head.psp_modules.2.1.bn.bias\",\n",
            "      \"decode_head.psp_modules.3.1.bn.weight\",\n",
            "      \"decode_head.psp_modules.3.1.bn.bias\",\n",
            "      \"decode_head.bottleneck.bn.weight\",\n",
            "      \"decode_head.bottleneck.bn.bias\",\n",
            "      \"decode_head.lateral_convs.0.bn.weight\",\n",
            "      \"decode_head.lateral_convs.0.bn.bias\",\n",
            "      \"decode_head.lateral_convs.1.bn.weight\",\n",
            "      \"decode_head.lateral_convs.1.bn.bias\",\n",
            "      \"decode_head.lateral_convs.2.bn.weight\",\n",
            "      \"decode_head.lateral_convs.2.bn.bias\",\n",
            "      \"decode_head.fpn_convs.0.bn.weight\",\n",
            "      \"decode_head.fpn_convs.0.bn.bias\",\n",
            "      \"decode_head.fpn_convs.1.bn.weight\",\n",
            "      \"decode_head.fpn_convs.1.bn.bias\",\n",
            "      \"decode_head.fpn_convs.2.bn.weight\",\n",
            "      \"decode_head.fpn_convs.2.bn.bias\",\n",
            "      \"decode_head.fpn_bottleneck.bn.weight\",\n",
            "      \"decode_head.fpn_bottleneck.bn.bias\",\n",
            "      \"auxiliary_head.conv_seg.bias\",\n",
            "      \"auxiliary_head.convs.0.bn.weight\",\n",
            "      \"auxiliary_head.convs.0.bn.bias\"\n",
            "    ],\n",
            "    \"lr_scale\": 1.0,\n",
            "    \"lr\": 0.00012,\n",
            "    \"weight_decay\": 0.0\n",
            "  }\n",
            "}\n",
            "2023-04-18 14:50:03,837 - mmseg - INFO - Loaded 4 images\n",
            "INFO:mmseg:Loaded 4 images\n",
            "2023-04-18 14:50:03,838 - mmseg - INFO - Start running, host: root@e7c2ba4103ad, work_dir: /content/ViT-Adapter/segmentation/work_dirs/upernet_deit_adapter_tiny_512_160k_ade20k\n",
            "INFO:mmseg:Start running, host: root@e7c2ba4103ad, work_dir: /content/ViT-Adapter/segmentation/work_dirs/upernet_deit_adapter_tiny_512_160k_ade20k\n",
            "2023-04-18 14:50:03,839 - mmseg - INFO - Hooks will be executed in the following order:\n",
            "before_run:\n",
            "(VERY_HIGH   ) PolyLrUpdaterHook                  \n",
            "(NORMAL      ) CheckpointHook                     \n",
            "(LOW         ) EvalHook                           \n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            " -------------------- \n",
            "before_train_epoch:\n",
            "(VERY_HIGH   ) PolyLrUpdaterHook                  \n",
            "(LOW         ) IterTimerHook                      \n",
            "(LOW         ) EvalHook                           \n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            " -------------------- \n",
            "before_train_iter:\n",
            "(VERY_HIGH   ) PolyLrUpdaterHook                  \n",
            "(LOW         ) IterTimerHook                      \n",
            "(LOW         ) EvalHook                           \n",
            " -------------------- \n",
            "after_train_iter:\n",
            "(ABOVE_NORMAL) OptimizerHook                      \n",
            "(NORMAL      ) CheckpointHook                     \n",
            "(LOW         ) IterTimerHook                      \n",
            "(LOW         ) EvalHook                           \n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            " -------------------- \n",
            "after_train_epoch:\n",
            "(NORMAL      ) CheckpointHook                     \n",
            "(LOW         ) EvalHook                           \n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            " -------------------- \n",
            "before_val_epoch:\n",
            "(LOW         ) IterTimerHook                      \n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            " -------------------- \n",
            "before_val_iter:\n",
            "(LOW         ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_val_iter:\n",
            "(LOW         ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_val_epoch:\n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            " -------------------- \n",
            "after_run:\n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            " -------------------- \n",
            "INFO:mmseg:Hooks will be executed in the following order:\n",
            "before_run:\n",
            "(VERY_HIGH   ) PolyLrUpdaterHook                  \n",
            "(NORMAL      ) CheckpointHook                     \n",
            "(LOW         ) EvalHook                           \n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            " -------------------- \n",
            "before_train_epoch:\n",
            "(VERY_HIGH   ) PolyLrUpdaterHook                  \n",
            "(LOW         ) IterTimerHook                      \n",
            "(LOW         ) EvalHook                           \n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            " -------------------- \n",
            "before_train_iter:\n",
            "(VERY_HIGH   ) PolyLrUpdaterHook                  \n",
            "(LOW         ) IterTimerHook                      \n",
            "(LOW         ) EvalHook                           \n",
            " -------------------- \n",
            "after_train_iter:\n",
            "(ABOVE_NORMAL) OptimizerHook                      \n",
            "(NORMAL      ) CheckpointHook                     \n",
            "(LOW         ) IterTimerHook                      \n",
            "(LOW         ) EvalHook                           \n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            " -------------------- \n",
            "after_train_epoch:\n",
            "(NORMAL      ) CheckpointHook                     \n",
            "(LOW         ) EvalHook                           \n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            " -------------------- \n",
            "before_val_epoch:\n",
            "(LOW         ) IterTimerHook                      \n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            " -------------------- \n",
            "before_val_iter:\n",
            "(LOW         ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_val_iter:\n",
            "(LOW         ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_val_epoch:\n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            " -------------------- \n",
            "after_run:\n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            " -------------------- \n",
            "2023-04-18 14:50:03,839 - mmseg - INFO - workflow: [('train', 1)], max: 400 iters\n",
            "INFO:mmseg:workflow: [('train', 1)], max: 400 iters\n",
            "2023-04-18 14:50:03,840 - mmseg - INFO - Checkpoints will be saved to /content/ViT-Adapter/segmentation/work_dirs/upernet_deit_adapter_tiny_512_160k_ade20k by HardDiskBackend.\n",
            "INFO:mmseg:Checkpoints will be saved to /content/ViT-Adapter/segmentation/work_dirs/upernet_deit_adapter_tiny_512_160k_ade20k by HardDiskBackend.\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py:3657: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\n",
            "2023-04-18 14:51:35,215 - mmseg - INFO - Iter [50/400]\tlr: 1.766e-06, eta: 0:10:37, time: 1.821, data_time: 1.085, memory: 10994, decode.loss_ce: 0.2847, decode.acc_seg: 5.9606, aux.loss_ce: 0.1105, aux.acc_seg: 5.9937, loss: 0.3951\n",
            "INFO:mmseg:Iter [50/400]\tlr: 1.766e-06, eta: 0:10:37, time: 1.821, data_time: 1.085, memory: 10994, decode.loss_ce: 0.2847, decode.acc_seg: 5.9606, aux.loss_ce: 0.1105, aux.acc_seg: 5.9937, loss: 0.3951\n",
            "2023-04-18 14:53:01,962 - mmseg - INFO - Iter [100/400]\tlr: 3.059e-06, eta: 0:08:53, time: 1.735, data_time: 1.121, memory: 10994, decode.loss_ce: 0.2203, decode.acc_seg: 25.3561, aux.loss_ce: 0.0999, aux.acc_seg: 10.8560, loss: 0.3202\n",
            "INFO:mmseg:Iter [100/400]\tlr: 3.059e-06, eta: 0:08:53, time: 1.735, data_time: 1.121, memory: 10994, decode.loss_ce: 0.2203, decode.acc_seg: 25.3561, aux.loss_ce: 0.0999, aux.acc_seg: 10.8560, loss: 0.3202\n",
            "2023-04-18 14:54:29,161 - mmseg - INFO - Iter [150/400]\tlr: 3.840e-06, eta: 0:07:21, time: 1.744, data_time: 1.120, memory: 10994, decode.loss_ce: 0.1583, decode.acc_seg: 29.7345, aux.loss_ce: 0.0911, aux.acc_seg: 24.1090, loss: 0.2493\n",
            "INFO:mmseg:Iter [150/400]\tlr: 3.840e-06, eta: 0:07:21, time: 1.744, data_time: 1.120, memory: 10994, decode.loss_ce: 0.1583, decode.acc_seg: 29.7345, aux.loss_ce: 0.0911, aux.acc_seg: 24.1090, loss: 0.2493\n",
            "2023-04-18 14:55:56,342 - mmseg - INFO - Iter [200/400]\tlr: 4.107e-06, eta: 0:05:52, time: 1.744, data_time: 1.116, memory: 10994, decode.loss_ce: 0.1203, decode.acc_seg: 33.3271, aux.loss_ce: 0.0883, aux.acc_seg: 31.7916, loss: 0.2086\n",
            "INFO:mmseg:Iter [200/400]\tlr: 4.107e-06, eta: 0:05:52, time: 1.744, data_time: 1.116, memory: 10994, decode.loss_ce: 0.1203, decode.acc_seg: 33.3271, aux.loss_ce: 0.0883, aux.acc_seg: 31.7916, loss: 0.2086\n",
            "2023-04-18 14:57:24,130 - mmseg - INFO - Iter [250/400]\tlr: 3.860e-06, eta: 0:04:23, time: 1.756, data_time: 1.124, memory: 10994, decode.loss_ce: 0.0984, decode.acc_seg: 34.6508, aux.loss_ce: 0.0815, aux.acc_seg: 34.0985, loss: 0.1800\n",
            "INFO:mmseg:Iter [250/400]\tlr: 3.860e-06, eta: 0:04:23, time: 1.756, data_time: 1.124, memory: 10994, decode.loss_ce: 0.0984, decode.acc_seg: 34.6508, aux.loss_ce: 0.0815, aux.acc_seg: 34.0985, loss: 0.1800\n",
            "2023-04-18 14:58:51,425 - mmseg - INFO - Iter [300/400]\tlr: 3.100e-06, eta: 0:02:55, time: 1.746, data_time: 1.113, memory: 10994, decode.loss_ce: 0.0814, decode.acc_seg: 33.3075, aux.loss_ce: 0.0694, aux.acc_seg: 32.8089, loss: 0.1508\n",
            "INFO:mmseg:Iter [300/400]\tlr: 3.100e-06, eta: 0:02:55, time: 1.746, data_time: 1.113, memory: 10994, decode.loss_ce: 0.0814, decode.acc_seg: 33.3075, aux.loss_ce: 0.0694, aux.acc_seg: 32.8089, loss: 0.1508\n",
            "2023-04-18 15:00:19,102 - mmseg - INFO - Iter [350/400]\tlr: 1.827e-06, eta: 0:01:27, time: 1.754, data_time: 1.120, memory: 10994, decode.loss_ce: 0.0876, decode.acc_seg: 36.7101, aux.loss_ce: 0.0717, aux.acc_seg: 36.3143, loss: 0.1594\n",
            "INFO:mmseg:Iter [350/400]\tlr: 1.827e-06, eta: 0:01:27, time: 1.754, data_time: 1.120, memory: 10994, decode.loss_ce: 0.0876, decode.acc_seg: 36.7101, aux.loss_ce: 0.0717, aux.acc_seg: 36.3143, loss: 0.1594\n",
            "2023-04-18 15:01:46,714 - mmseg - INFO - Saving checkpoint at 400 iterations\n",
            "INFO:mmseg:Saving checkpoint at 400 iterations\n",
            "2023-04-18 15:01:48,772 - mmseg - INFO - Iter [400/400]\tlr: 4.096e-08, eta: 0:00:00, time: 1.793, data_time: 1.119, memory: 10994, decode.loss_ce: 0.0706, decode.acc_seg: 34.1671, aux.loss_ce: 0.0624, aux.acc_seg: 33.4926, loss: 0.1330\n",
            "INFO:mmseg:Iter [400/400]\tlr: 4.096e-08, eta: 0:00:00, time: 1.793, data_time: 1.119, memory: 10994, decode.loss_ce: 0.0706, decode.acc_seg: 34.1671, aux.loss_ce: 0.0624, aux.acc_seg: 33.4926, loss: 0.1330\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "brmf6eCtwL7O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e18fadf2-11ce-4196-8463-7faed5f1d86d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ViT-Adapter/segmentation\n",
            "2023-04-18 15:02:18,206 - mmseg - INFO - Loaded 4 images\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/content/ViT-Adapter/segmentation/mmseg_custom/models/losses/cross_entropy_loss.py:230: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with PyTorch official cross_entropy, set ``avg_non_ignore=True``.\n",
            "  warnings.warn(\n",
            "load checkpoint from local path: work_dirs/upernet_deit_adapter_tiny_512_160k_ade20k/iter_400.pth\n",
            "[                                                  ] 0/4, elapsed: 0s, ETA:/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py:3657: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\n",
            "[>>] 4/4, 0.9 task/s, elapsed: 4s, ETA:     0sper class results:\n",
            "\n",
            "+------------+-------+-------+\n",
            "|   Class    |  IoU  |  Acc  |\n",
            "+------------+-------+-------+\n",
            "|   sclera   | 99.15 | 99.15 |\n",
            "| background |  0.0  |  nan  |\n",
            "+------------+-------+-------+\n",
            "Summary:\n",
            "\n",
            "+-------+-------+-------+\n",
            "|  aAcc |  mIoU |  mAcc |\n",
            "+-------+-------+-------+\n",
            "| 99.15 | 49.58 | 99.15 |\n",
            "+-------+-------+-------+\n"
          ]
        }
      ],
      "source": [
        "%cd /content/ViT-Adapter/segmentation\n",
        "!python test.py configs/ade20k/upernet_deit_adapter_tiny_512_160k_ade20k.py work_dirs/upernet_deit_adapter_tiny_512_160k_ade20k/iter_400.pth --eval mIoU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcSILsHUL_19",
        "outputId": "527fd39f-ed66-400b-89a1-3e8c2ed7accf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ViT-Adapter/segmentation/mmseg_custom/models/losses/cross_entropy_loss.py:230: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with PyTorch official cross_entropy, set ``avg_non_ignore=True``.\n",
            "  warnings.warn(\n",
            "load checkpoint from local path: work_dirs/upernet_deit_adapter_tiny_512_160k_ade20k/iter_100.pth\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: cHRM chunk does not match sRGB\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py:3657: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: cHRM chunk does not match sRGB\n",
            "/usr/local/lib/python3.9/dist-packages/mmseg/models/segmentors/base.py:275: UserWarning: show==False and out_file is not specified, only result image will be returned\n",
            "  warnings.warn('show==False and out_file is not specified, only '\n",
            "Result is save at demo/ADE_val_00000003.png\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python image_demo.py \\\n",
        "  configs/ade20k/upernet_deit_adapter_tiny_512_160k_ade20k.py  \\\n",
        "  work_dirs/upernet_deit_adapter_tiny_512_160k_ade20k/iter_100.pth \\\n",
        "  data/ade/images/validation/ADE_val_00000003.png \\\n",
        "  --palette ade20k"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}